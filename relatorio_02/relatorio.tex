\documentclass[10pt,twocolumn]{article}

\usepackage{graphicx,url}
\usepackage{subfigure}

\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{subfigure}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Header  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Trabalho prático 2}
\author{Tiago Alves Macambira \\ \texttt{tmacam@dcc.ufmg.br}}
\date{6 de julho de 2007}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Body  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Introdução}

O segundo trabalho prático da disciplina de Recuperação de Informação
consiste no projeto e implementação de programas para
recuperação eficiente de grandes base de dados. Tal sistema deve
 atender vários requisitos, como disposto no
enunciado do trabalho~\cite{tp1}, dentre os quais podemos citar:
\begin{itemize}
\item O sistema deve utilizar como base os dados coletados pelo
\emph{crawler} do TP1,
\item O sistema deve ser capaz de responder \emph{queries} com uma ou
mais palavras e intepretar o uso dos operadores booleanos \texttt{AND}
e \texttt{OR}.
\item O sistema deve utilizar arquivos invertidos como mecanismo
de índice.
\item O uso de mecanismos para compactação de índice, apesar de
opcional, é recomendado.
\item O sistema deve ser escrito em C/C++. Bibliotecas extras somente
poderão ser utilizadas mediante autorização dos professores.
\end{itemize}

Nesse relatório, comentaremos sobre a resolução do nosso trabalho:
os desafios encontrados, as escolhas de projetos
adotadas e peculiaridades de nossa implementação. Além disso, comentamos
sobre o desempenho do mesmo, as estruturas de dados usadas bem como suas
complexidades.

Muitos dos pontos abordados no trabalho prático anterior são também
relevantes a esse trabalho prático. Por esse motivo e para tornar a
leitura desse relatório independente da leitura do relatório anterior,
sempre que necessário, utilizaremos fragmentos do relatório anterior.

\section{Indexação de páginas web}

Dado um conjunto de páginas \emph{Web} coletadas, para que seja possível
realizar buscas eficientemente nessa coleção é necessário, antes de mais
nada, indexar o seu conteúdo.

De forma resumida, a indexação tanto de conteúdo \emph{Web} bem como 
de coleções de texto no geral possui os seguintes passos:

\begin{description}

\item[Reconhecimento da estrutura:] No caso de documentos HTML, essa
etapa corresponde ao \emph{parsing} dos documentos da coleção e a
extração dos fragmentos de texto existentes em cada documento.

\item[Normalização de caso e acentos:] Para o propósito de indexar
documentos, não faz sentido diferenciar letras maiúsculas de letras
minúsculas em palavras. Além disso, é comum também que se ignore a
presença de acentos em palavras, de forma a considerar, por exemplo,
 as palavras ``\texttt{ação}'' e ``\texttt{acao}'' como sendo iguais.

\item[Remoção de \emph{stopwords}:] Algumas palavras tais como
preposições e conjunções não agregam valor semântico ao texto e, em
teoria, a remoção destas palavras não acarretaria danos ao processo de
indexação. Além disso, devido a freqüência dessas palavras nos textos,
sua remoção também proporciona economia no tamanho do índice. Todavia,
alguns trabalhos argumentam que, no contexto de máquinas de busca para a
\emph{Web}, não faz sentido retirar \emph{stopwords} durante a
indexação, sob pena de prejudicar o processamento de
frases~\cite{moffat1999managing}.

\item[\emph{Steeming}:] A idéia é a de armazenar apenas os radicais das
palavras no índice, facilitando assim as buscas por flexões da mesma
palavra e, novamente, diminuindo o tamanho o mesmo.

\item[Geração do Índice:] Finalmente, a última etapa da indexação é a
geração propriamente dita das estruturas e do conteúdo do índice. 

\end{description}

\subsection{Problemas e desafios}

Nessa seção comentaremos sobre alguns dos problemas que podem ser
encontrados durante a indexação de conteúdo HTML. Na
seção~\ref{sec:implementation} comentaremos como lidamos com esses
problemas na nossa implementação.

\subsubsection{Normalização de mapas de
caracteres}\label{prob:charmapnorm}

As diferentes línguas utilizadas pelo homem usam distintos conjuntos de
símbolos para sua representação e, para vários desses conjunto de
símbolos, mais de uma forma de representação binária existe. Esse fato
torna complicado a troca e a interpretação de texto em línguas diferentes
e mesmo para documentos escritos na mesma língua. Para a língua inglesa,
por exemplo, que utiliza um conjunto de caracteres bem limitado, existem
dois conjuntos de mapas de caracteres históricos diferentes e
incompatíveis, ASCII e EBCDIC. Se contarmos os mapas de caracteres e
formas de codificação diferentes existentes para as várias línguas
humanas, esse problema torna-se bem mais complexo.
Mesmo a existência de conjuntos ``universais'' como UTF-8, UTF-16 e
UTF-32 não resolvem esse problema, pois nem todos os documentos na
Internet se encontram codificados nesses mapas de caracteres.

É salutar observar que a capacidade de interpretar corretamente
documentos escritos em mapas de caracteres distintos e de recodificá-los
para um mapa de caractere comum  repercute não somente na capacidade de
realizar \emph{parsing} de documentos, mas também na capacidade de
indexação e extração de \emph{links} do mesmo, uma vez a forma de
representar caracteres irá afetar diretamente a capacidade de
normalizar o vocabulário e de normalizar URLs.

\subsubsection{Descoberta de mapa de
caracteres}\label{prob:charmapdetection}

Associado ao problema de normalizar os documentos em um único mapa de
caracteres está o problema de, dado um documento, descobrir em qual mapa
de caracteres o mesmo se encontra para que se possa então realizar a
tradução. Informações sobre o mapa de caractere usado por uma página
\emph{Web} podem ser fornecidas de diversas formas:
\begin{itemize}
\item por cabeçalhos da requisição HTTP~\cite{rfc2616},
 o protocolo de aplicação utilizado na \emph{Web},
\item pelo prólogo de um documento XML~\cite{bray2006xml},
\item através de uma \emph{tag} \texttt{Meta} em um documento
(x)HTML~\cite{html4tr}.
\end{itemize}

Entretanto, os padrões acima apresentam sobreposições, colocando o
desenvolvedor em situações de impasse. Por exemplo, quando mais de um
padrão fornece informações conflitantes, qual utilizar?  Quando nenhum
mecanismo acima informa nada sobre o mapa de caracteres usado, qual
padrão ou qual regra usar por omissão?

Uma alternativa plausível é realizar o mesmo processo utilizado por
navegadores \emph{Web}, que é uma extensão do modelo proposto pela RFC
3023~\cite{rfc3023}. Esse processo consiste em seguir os passos abaixo,
parando no primeiro que tenha suas restrições atendidas:
\begin{enumerate}
\item Usar as informações dos cabeçalhos HTTP, sempre que possível e
sempre que estes estejam disponíveis.
\item  Se os primeiros 2--4 bytes forem macadores de ordem de bytes
(BOM, de \emph{Byte Order Mark}) XML ou UTF, a codificação
correspondente será utilizada.
\item Se o documento iniciar com um prólogo XML legível e que informe a
codificação do documento, esse será utilizado.
\item Se o documento conter a tag HTML \texttt{meta}, como em \texttt{<meta
http-equiv="Content-Type" ...> }, o mapa de caracteres declarado nessa
tag será usado.
\item Finalmente, como última opção, existe a abordagem de detecção de
mapas de caracteres por heurísticas compostas~\cite{mozillaiuc}.
\end{enumerate}

\subsubsection{\emph{Parsing} de páginas HTML}\label{prob:html}

Apesar de já aparecer no contexto de extração de \emph{links} (TP1),
esse problema  possui uma maior relevância no processo de indexação
de documentos.

Os documentos HTML existentes na \emph{Web} atual geralmente não são
considerados válidos
e/ou bem formados se confrontados com os padrões existentes e publicados
pela W3C para HTML e XHTML~\cite{html4tr, bray2006xml}, sendo comumente
uma mistura dos dois padrões. A utilização de estratégias simples para
\emph{parsing} do conteúdo HTML como
através do uso de expressões regulares, se eficiente,  mostra-se ingênua
quando deparada com documentos HTMLs mais complexos. Não somente uma
abordagem dessas pode ignorar seções como comentários HTML/XML, mas
também ignorar \emph{tags} especiais do HTML tais como a \texttt{SCRIPT},
\texttt{TEXTAREA} e \texttt{STYLE}, que, se presentes, podem conter
conteúdo que não é HTML e que não deve ser indexado.

\subsubsection{\emph{Parsing} de entidades}

Observe que o \emph{parsing} de documentos HTML para a indexação não se
restringe a retirar destes os fragmentos de texto presentes observando
apenas as características estruturais do documento, mas também consiste
em realizar, nos fragmentos de texto encontrados, a conversão de
referência de caracteres e referências de entidades para representações
textuais das mesmas. Esse processo só deve ser feito após a extração dos
fragmentos de texto, conforme indicado na especificação de XML, e
pressupõe a capacidade de converter \emph{codepoints} Unicode
arbitrários para suas respectivas representações no mapa de caracteres
em uso pelo parser~\cite{bray2006xml}.

\subsubsection{Internacionalização e \emph{tokenização} de
texto}\label{prob:token}

O conteúdo existente na \emph{Web} pode estar representado nas mais
diversas línguas. Como mencionado acima, diversas formas de representar
e diversas opções de normalização de mapas de caracteres existem. Se o
intuito é evitar a perda de informação, é mais interessante armazenar os
documentos em alguma codificação de Unicode, tais como UTF-8 e UTF-32,
entre outras. Todavia, as várias opções existentes para formas de
representar dados com Unicode apresentam diversas opções de
compromisso.

Se por um lado representações \emph{multi-byte} como UTF-8 e UTF-16
permitem o armazenamento de conteúdo de línguas latinas em menor espaço,
essas representações dificultam significativamente o trabalho de separar
o texto em \emph{tokens} ou ``\emph{tokenizar}''. Considere, por
exemplo, o texto ``\emph{ação«perdida»}''. Todos os caracteres gráficos
e todas as letras acentuadas no exemplo, quando codificados em UTF-8,
utilizarão 2 caracteres. Esse texto, em UTF-8 é representado como
\begin{small}
\begin{verbatim}a\xc3\xa7\xc3\xa3o\xc2\xabperdida\xc2\xbb\end{verbatim}.
\end{small}
A menos que o
tokenizador ``interprete'' os símbolos multi-byte, não será possível
para ele diferenciar um byte que faz parte de uma seqüência
multi-byte de uma letra de um que faz parte de uma seqüência multi-byte
de um símbolo ou sinal de pontuação, o que inviabilizaria a tokenização.

Representações que usam \emph{wide-char}, como UTF-32, facilitam
significativamente o trabalho de tokenizar o texto, uma vez que cada
\emph{codepoint} Unicode ocupa apenas um único \emph{wide-char}, de
tamanho definido e constante. Por outro lado, existe um grande
desperdício de espaço no uso desse tipo de representação de dados em
Unicode.



\subsubsection{Compressão de Índice}

Existem diversos métodos para comprimir o índice gerado (
Ellias-\(\gamma\), Ellias-\(\delta\), codificações
\emph{byte-wise}, etc)~\cite{moffat1999managing,moffat2006survey}.
Dependendo do tamanho do índice gerado e das suas limitações de espaço
e/ou de desempenho, pode ser necessário usar algum desses métodos no
processo de escrita e de leitura do índice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROJETO %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{O projeto do indexador}

Realizamos o projeto de nosso indexador tendo em mente os desafios
mencionados na seção anterior e algumas decisões tomadas durante o
trabalho prático 1 (TP1). Algumas decisões de projeto tiveram que
ser tomadas tanto para contornar tais problemas como para ser capaz de
completar o trabalho num prazo razoável.


\subsection{Premissas e decisões iniciais de projetos}

Uma das primeiras decisões tomadas, ainda no TP1, foi sobre qual mapa de
caractere seria utilizado na normalização dos dados coletados.  Tendo em
vista os pontos discutidos na sub-seção~\ref{prob:charmapnorm} e a
despeito dos pontos lançados em~\ref{prob:token}, decidimos que
\textbf{todo o conteúdo de páginas HTML coletadas será normalizado para
UTF-8}. O problema de descoberta de mapas de caracteres foi contornado
durante a coleta das páginas.

Outra opção de projeto foi sobre o que seria indexado e como seria o
nosso tratamento de \emph{stopwords}. Como recomendado nos artigos e
livros sugeridos na ementa da disciplina para indexação de conteúdo
\emph{Web}, \textbf{indexamos toda e qualquer palavra que apareça} na
nossa coleção, sem realizar nenhum tratamento especial para
números~\cite{moffat1999managing}.

\textbf{Não realizaremos \emph{stemming}} das palavras.

\textbf{Realizaremos compressão do índice}. Todavia, ao invés de
utilizar os métodos sugeridos em sala, utilizaremos um \textbf{método de
codificação \emph{byte-wise}} tanto para codificação de \emph{d-gaps}
como para codificar as freqüências \(f_{d,f}\) nas listas invertidas.
Esse método de compressão apresenta ganhos menores que os obtidos com
codificações orientadas a bits mas ainda assim seus ganhos são
significativos e essa codificação apresenta um desempenho melhor tanto
no processo de compressão como no processo de \emph{query
evaluation}~\cite{zobel2002fastquery}.

Assumimos que \textbf{todo o vocabulário cabe na memória}. Isso é necessário
tanto para a indexação como para o processamento de consultas.

\subsection{Componentes}


Os componentes da no nosso \emph{indexador} são:
\begin{description}
\item[armazém de dados:] uma fina camada sobre o sistema de arquivos que
permite organizar os dados coletados/indexados;
\item[indexador:] realiza o \emph{parsing} dos documentos e escreve
\emph{runs};
\item[merger:] realiza o \emph{n-way merging} das runs salvas e escreve
o arquivo invetido comprimido;
\item[querybool:] nosso processador de consultas booleanas.
\end{description}


Observe que o processador de consultas não faz parte do processo de
indexação propriamente dito, mas faz parte da especificação do trabalho.

Maiores detalhes sobre a implementação deles serão fornecidos na seção
seguinte.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPLEMENTAÇÃO %%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementação}\label{sec:implementation}

\subsection{Decisões de implementações}

Nosso indexador foi escrito em C++
e utiliza a biblioteca de gabaritos padrões (STL) dessa
linguagem~\cite{stroustrup97}.
Não foi utilizada nenhuma nova libioteca em comparação às já utilizadas
no TP1.

Além disso, nosso código faz uso extensivo de uma técnica para controle
de recursos (como memória, \emph{locks} etc)  muito comum em C++,
denominada RAII (\emph{Resource Acquisition Is Initialization}).
Diversas partes de nosso código possuem código de teste (\emph{Unit
Testing}).

Além disso sempre que possível, delegamos o processo de leitura de
arquivos ao SO através do uso de \texttt{mmaps}.

\subsection{Estruturas de armazenamento}\label{sec:storage}

A forma como que os dados são gravados e recuperados do disco é
tão importante quanto os algoritmos e estruturas de dados utilizados
pelo nosso indexador. As estruturas de armazenamento podem ditar ou
limitar a efetividade dos algoritmos utilizados.

Em nossa solução, vocabulário e arquivo invertido possuem uma estrutura
de armazenamento similar e composta, de maneira geral, de dois
elementos:
\begin{description}

\item[Arquivo-cabeçalho] Uma estrutura que permite acesso rápido a
informações sobre um determinado termo ou documento.  Tais informações
consistem, entre outras coisas, na localização (\emph{offset} ou
ponteiro) do conteúdo de interesse no arquivo(s) de dados . \footnote{
Na prática, esse arquivo nada mais é do que o \emph{dump} de um
\emph{array}, o que, ao utilizarmos técnicas como \texttt{mmap}, permite
o aceso quase que em tempo constante à informação desejada.}

\item[Arquivo(s) de dados] Nesse arquivo, termos do vocabulário ou listas
invertidas propriamente ditas são armazenadas concatenadas. A
localização de um item em um arquivo de dados depende da leitura do
arquivo-cabeçalho correspondente.
\end{description}

No caso do vocabulário, o arquivo de dados consiste na concatenação das
palavras do vocabulário em ordem ascendente de seus \(t_{id}\).

As listas invertidas podem, quando concatenadas, atingir um tamanho
superior ao tamanho máximo de um arquivo no SO utilizador. Para evitar
problemas, os dados das listas invertidas são distribuídos entre vários
arquivos de dados, cada um de aproximadamente 256~MB. A localização de
uma lista invertida relativa a um termo em um destes arquivo de dados
bem como a informação de em qual arquivo de dados essa lista se encontra
é guardada no arquivo-cabeçalho do arquivo invertido. 

\subsection{Estruturas de dados}\label{sec:datastructures}

Nossa implementação usa as seguintes estruturas de
dados~\cite{cormen-algorithms}:

\begin{itemize}

\item Na indexação, utiliza-se uma \emph{hashtable} tanto para armazenar
o vocabulário (termo\(\rightarrow t_{id}\)) como para obter a representação
textual em UTF-8 de referências para entidades. No processamento de
consulta, também se utiliza uma \emph{hashtable} como nos moldes acima
para armazenar o vocabulário.  Busca e inserção nessa estrutura possui
uma complexidade de tempo médio de \(O\left(1 \right)\).

\item Uma fila de prioridades é utilizada no processo de \emph{n-way
merging}. Essa fila é implementada como uma \emph{heap} de ``leitores de
\emph{runs}'' e os elementos dela são ordenados pelo valor da tripla
\(<t_{id}, d_{id}, f_{d,t}>\) disponível para leitura em cada um dos
``leitores de \emph{run}''.  Inserção, e remoção nessa estrutura possuem
complexidades de tempo no pior caso de \(O\left(\log n\right)\). A
localização do elemento de maior prioridade leva tempo constante.  O
processo completo de intercalar as \emph{runs} é da ordem de \(O\left(
n\times\log r\right)\) , onde \(r\) é o número de leitores de
\emph{runs} em uso.

\item Cada leitor de \emph{run} apresenta tempo constante para obtenção
do seu próximo valor.

\item Durante o processo de indexação e geração da \emph{runs}, várias
triplas são armazenadas em memória até que se atinja um volume
pré-determinado (512~MB). Atingido esse valor, essas triplas são
ordenadas com um algoritmo \emph{introsort}\footnote{ D. R. Musser,
"Introspective Sorting and Selection Algorithms", Software Practice and
Experience 27(8):983, 1997.  } cuja complexidade de pior caso é
\(O\left(n \log n\right)\)

\end{itemize}

\subsection{Componentes}

Nesta seção discutiremos decisões de implementação, problemas e soluções
encontradas no desenvolvimento de cada um dos componentes da nossa
solução.

\subsubsection{\emph{Parsers}}

Para extrair texto dos documentos HTML e para converter texto com
entidades em texto puro, aproveitamos a pequena infra-estrutura para
a construção de \emph{parsers} recursivos-descendentes
(\texttt{BaseParser}) criada para o TP1. Essa implementação busca evitar cópias
desnecessárias de dados ao fazer uso extensivo de uma estrutura própria
(\texttt{filebuf}) em lugar de strings C++ convencionais.

\subsubsection{\emph{Parsing} de documentos HTML}

O extrator de texto de documentos HTML foi é construído sobre um
\emph{parser} genérico para documentos XML e HTML, o
\texttt{SloppyHTMLParser}. Esse parser é na verdade um
\emph{parser} completo e versátil para documentos HTML e XML. Sua
elaboração foi feita tendo em vista as especificações dos dois padrões
de documentos acima e o comportamento que navegadores \emph{Web}
tradicionais apresentam quando defrontados com documentos mal-formados.
A interface provida para seus utilizadores é similar à de \emph{parsers}
SAX XML tradicionais~\cite{saxxml, bray2006xml, html4tr}. Ao contrário
destes últimos, ele e é capaz de lidar com uma ampla variedade de
construções XML e HTML mesmo em documentos
mal-formados. Além disso, ele reconhece \emph{tags} HTML que devem ter
seu conteúdo ignorado por \emph{parsers} desse tipo de documento, como
comentado na seção~\ref{prob:html}.

\subsubsection{Detecção e Normalização de Mapas de Caracteres}

Como dito anteriormente, a detecção e normalização de mapas de
caracteres foi feita durante o processo de coleta utilizando o
componente componente \texttt{UnicodeBugger} Todo
o conteúdo coletado, antes de ser processado, foi convertido para UTF-8
por instâncias desse componente, resolvendo os problemas comentados na
seção~\ref{prob:charmapnorm}.

O processo de detecção de mapas de caracteres segue os passos descritos
anteriormente na seção na seção~\ref{prob:charmapdetection},
excetuando-se pelo último deles, o de heurísticas compostas, por ser
muito dispendiosa~\cite{mozillaiuc}.


\subsubsection{Normalização de Termos}

Durante o processo de indexação e durante o processamento de consultas
realizamos a normalização dos termos (indexados e termos da consulta).
A normalização possui os seguintes passos:
\begin{enumerate}
\item Conversão de texto de UTF-8 para UTF-32
\item Tokenização de conteúdo em UTF-32
\item Conversão das letras de cada palavra encontrada para minúsculo.
\item Conversão de caracteres acentuados para seus respectivos
caracteres sem acento
\item conversão de texto em UTF-32 para UTF-8
\end{enumerate}

Dessa forma, nosso indexador e nosso processador de consulta  não faz
diferença entre os termos \texttt{AçãO} e \texttt{acao}. Observa-se que
um processo com efeitos similares ocorre em máquinas de buscas como o
Google. 

\subsubsection{Leitores, escritores e intercaladores de \emph{run}}

Tentamos modelar nossos leitores, escritores e intercaladores de
\emph{runs} como iteradores C++. Desta forma, torna-se mais fácil
integrar o código existente com os algoritmos da C++ que esperam
interadores.


%\subsection{Dificuldades}
%
%Nessa sub-seção discutiremos alguns dos problemas enfrentados durante a
%elaboração e teste da nossa implementação.
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Testes e Resultados}

\subsection{Ambiente experimental}

Todos os testes de nossa implementação foram realizados em um micro do
laboratório de graduação do DCC/UFMG, rodando como S.O. Linux Gentoo
1.12.9, kernel 2.6.19-gentoo-r5, dotado de uma CPU Pentium 4 de
3.00~MHz, 1~GB de memória RAM e 1~GB de \emph{swap}.

\subsection{Resultados}

Documentos/s : 20.568
KB/s : 562


\begin{table}[htbp]
\centering
\begin{tabular}{|c|r|} \hline
Volume do vocabulário&	      29.948 KB \\\hline
Volume da base de &   			\\
dados (compactado)&   	  13.977.374 KB \\\hline
Volume das runs  &	   7.180.092 KB \\\hline
Volume do índice  &			\\
  sem compressão  &	   5.764.740 KB \\
  com compressão  &	   1.547.700 KB \\\hline
Tempo para geração do índice  &		\\
  sem compressão  &	   9m43.120s \\
  com compressão  &	   7m12.012s \\\hline
Velocidade de indexação&		\\
  doc/s		  & 20,568		\\
  KB/s		& 562			\\\hline
Número de Termos&     2.311.837	\\\hline
Número de documentos& 1.572.822		\\\hline
\hline
\end{tabular}
\caption{Dados sobre as coletas}
\label{tab:sumario}
\end{table}


A Tabela~\ref{tab:sumario} apresenta um sumário dos valores de
desempenho e de outras informações pertinentes sobre esse TP.

Na Figura~\ref{fig:desempenho}, podemos acompanhar o desempenho do
processo de indexação. Na sub-figura~\ref{fig:bytes} vemos o gráfico da
da variação da taxa de indexação medida em KBytes. Já na
sub-figura~\ref{fig:docs} vemos o crescimento do volume acumulado de
documentos indexados no decorrer do tempo.

\begin{figure*}
  \centering
  \mbox{
    \subfigure[Bytes indexados/s]{
          \label{fig:bytes}
          \includegraphics[width=0.45\textwidth]{plot_bps}}
 }
 \mbox{
    \subfigure[Documentos indexados]{
          \label{fig:docs}
          \includegraphics[width=0.45\textwidth]{plot_dps}}
  }
  \caption{Desempenho do indexador}
  \label{fig:desempenho}
\end{figure*}


Observe que os gráficos apresentados na Figura~\ref{fig:desempenho} são
referentes a uma execução do processo de indexação diferente daquela
apresentada na Tabela~\ref{tab:sumario}. A execução cujos dados estão
apresentados nessa tabela dizem respeito a idenxação de todo o conteúdo
coletado. A existência dessa diferença entre a tabela e os dados ocorre
devido ao fato de que as alterações necessárias no indexador para a
geração desse gráfico foram feitas tardiamente e uma re-execução
completa do processo de indexação levaria aproximadamente mais 15,5
horas.\footnote{E esse TP já está atrasado.}

\subsection{Exemplo de uso}

Podemos ver saídas de execuções do processador de consultas nas
Figuras~\ref{fig:exemplo} e \ref{fig:real}. A primeira presenta a saída
de uma execução na qual foi utilizada uma coleção de dados de exemplo
igual a coleção usada em sala de aula nas aulas de indexação. A segunda
apresenta a saída para algumas \emph{queries} utilizando o índice gerado
a partir dos dados coletados.

\begin{figure*}
\begin{center}
\begin{verbatim}
./querybool ../test_indexer/
Loading vocabulary and inverted file ... done
Type your query using AND, OR and spaces to split terms.
Default operation is AND (conjunctive).
Notice: a AND b c OR d == (((a AND b) AND c) OR d )
> porridge
Document(s) found matching query 'porridge':  1 2
> hot
Document(s) found matching query 'hot':  1 4
> hot AND porridge
Document(s) found matching query 'hot AND porridge':  1
> hot OR porridge
Document(s) found matching query 'hot OR porridge':  1 2 4
>
\end{verbatim}
\caption{Uso da ferramenta com uma base de exemplo}
\label{fig:exemplo}
\end{center}
\end{figure*}

\begin{figure*}
\begin{center}
\begin{verbatim}
$ ./querybool ../index_data
Loading vocabulary and inverted file ... done
Type your query using AND, OR and spaces to split terms.
Default operation is AND (conjunctive).
Notice: a AND b c OR d == (((a AND b) AND c) OR d )
> lésbicas OR selvagens AND freiras
Document(s) found matching query 'lésbicas OR selvagens AND freiras':  31922
65842 84882 94838 108508 115584 124033 130857 141172 158426 168044
183336 196840 207264 216442 226165 236593 243890 252612 256096 263363
272922 282825 290036 299370 307725 315274 323257 332230 341223 349546
356752 364623 373471 382413 390689 409396 426570 439908 449507 456076
462829 469155 475139 480537 487507 496218 502690 512025 518657 524208
531037 540090 544070 549361 555214 560007 565528 568628 570142 574648
579555 585285 591063 592813 604989 610850 615952 625775 630188 632206
641719 646877 651394 658620 663791 663793 663953 669615 674586 680257
682757 687234 693030 698780 704742 709824 715349 720210 724919 731019
737507 737589 743592 748693 754156 756055 758788 760390 760392 763375
769037 773114 777792 780059 781749 785801 789528 791333 794381 799364
804656 809995 817204 826368 828038 830201 834120 838065 849036 854318
861354 865075 869356 873638 886580 891076 896460 902930 903463 910369
914225 920268 924571 930078 935533 939771 944401 949906 953827 959028
963400 967952 972054 976207 980077 984419 989587 994954 999051 1004041
1006623 1010623 1011280 1015995 1016813 1021027 1026184 1030933 1041424
1045448 1051886 1057097 1057299 1061454 1065080 1065520 1070027 1074334
1079421 1083479 1086813 1087051 1091880 1092497 1099165 1172010 1172012
1208717 1215003 1225674 1231362 1237443 1242387 1253852 1260545 1266924
1271244 1277691 1283726 1289641 1296986 1302666 1304467 1307215 1313336
1318746 1324809 1329855 1335839 1340520 1342909 1346002 1350950 1355268
1361052 1366442 1373461 1385372 1390943 1396249 1401501 1406075 1411560
1417425 1423514 1427359 1429289 1433270 1438565 1442471 1447409 1451165
1455432 1461749 1466591 1470365 1473606 1477900 1481885 1485432 1488742
1492894 1496786 1501076 1501246 1505783 1508451 1510345 1510486 1514772
1518651 1523382 1527498 1533617 1537628 1540675 1544839 1548062 1551580
1552308 1878118 1914096 2172227 2172229 2520734 2566735 2566737 2647844
2665235 2953721 3080068 3704516 3726904 4307367
> freiras hentai
Document(s) found matching query 'freiras hentai':  5090591
> psicanalistas AND psicopatas
Document(s) found matching query 'psicanalistas AND psicopatas':  1940391
2091678
>
\end{verbatim}
\caption{Uso da ferramenta com dados reais}
\label{fig:real}
\end{center}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Conclusões}
%
%Nesse relatório descrevemos a implementação do \emph{crawler}
%especificado no TP1 da disciplina~\cite{tp1}, bem como os problemas
%enfrentados e as decisões de projeto tomadas.
%
%Pudemos observar como a política de escalonamento possui um papel
%fundamental no desempenho do coletor. Em nossos experimentos, o
%uso da política ``\emph{largest site first}'' mostrou-se capaz de, em
%50\% do tempo necessário, baixar 150\% do conteúdo desejado para um
%único dia de coleta. Além disso, mostro-se quase duas vezes mais eficiente do que a
%política ``\emph{breadth-first}''. Esses resultados, embora bastante
%interessantes, estão longe de ser novidades, sendo bastante estudados na
%literatura~\cite{baezayates2005crawling}.
%
%
%Finalmente, foi também possível observar como que, a despeito de ser
%aparentemente um problema simples, a elaboração de um \emph{crawler}
%industrial é complicada.
%
%\subsection{Problemas encontrados e Melhorias
%futuras}\label{sec:problemas}
%
%Durante a implementação do \emph{crawler}, diversos problema foram
%observados mas, devido a problema relativos ao tempo de entrega do
%trabalho, tiveram que ser deixados de lado. Todavia, esses problemas
%merecem menção, uma vez que constituem limitações da implementação e
%que, em TP's futuros podem se revelar como problemas cuja solução será
%necessária.
%
%\subsubsection{Armadilhas para \emph{crawlers}}\label{dif:armadilhas}
%
%Um dos problemas de se lidar com a coleta de conteúdo dinâmico é que
%sítios com esse tipo de página pode conter o que se chama e ``buracos
%negros'', ou seja, é possível que o coletor possa se perder ou passar
%mais tempo do que o necessário tentando coletar uma
%infinidade de páginas geradas dinamicamente. Ignorar o conteúdo do
%componente \emph{query} de URLs é uma forma de tentar contornar esse
%problema mas está longe de ser uma solução eficaz, uma vez que existem
%formas de se apresentar conteúdo dinâmico sem que esse envolva o uso de
%URLs com componentes \emph{query}.
%
%Uma outra forma de gerar conteúdo ou URLs dinâmicas é através do
%componente \emph{domainname} de URLs. Essa forma é particularmente
%nociva à nossa implementação, sobretudo na que usa a política de
%escalonamento BF. Isso deve-se a diversos fatores:
%\begin{itemize}
%\item Quando uma página em um domínio desses é encontrada pela primeira
%vez, acaba-se descobrindo por tabela diversas páginas em diversos
%domínios dinâmicos pertencentes ao mesmo sítio;
%\item uma vez que cada novo domínio gerado
%acarreta a criação de uma instância de gerente de domínio em nossa
%implementação, ao visitar apenas uma única página num sítio desses
%diversas instâncias de gerentes de domínios terão de ser geradas
%instantaneamente,
%\item finalmente,  como nessa política os domínios são visitados por ordem de
%descoberta, o inicio da coleta do primeiro domínio dinâmico de um sítio
%desses acarretará diversas requisições DNS, uma para cada um dos outros
%domínios dinâmicos encontrados em seqüência.
%\end{itemize}
%
%Nessas circunstâncias o desempenho do coletor cai para patamares
%baixíssimos, como pode-se observam em alguns vales na
%Figura~\ref{fig:bf}. A política de escalonamento LSF contorna isso de
%maneira elegante, mas até que ponto? Se tivessemos deixado o coletor com
%LSF rodando por mais tempo teríamos encontrado esse tipo de
%comportamento?
%
%%Buracos negros - nossa abordagem de escalonamento foi baseada em domínios. Isso
%%permitia não somente controlar o tempo mínimo de intervalo entre acessos
%%consecutívos a um mesmo domínio, limitar o uso de memória (já que nao era mais
%%necesári armazenar o nome de domínio em cada URL pertencente a ele) e armazenar
%%a lista de URLs pendentes. Todavia, para a nossa implementação, sítios que
%%fazem uso extensivo de sub-domínios para sua organização são torna um problema.
%%Isso deve-se ao fato de que, custumeiramente, a obtenção e \emph{parsing} de
%%uma unica página de um sítio destes acarreta na ``descoberta'' de vários
%%sub-domínos desse sítio, o que por sua vez acarreta no acréscimo de cada um
%%destes na lista de domínios conhecidos e de domínios pendentes. Além desses
%%domínios serem costumeriramente rasos, ou seja, sem muitos links para
%%documentos internos, a sua adição limitará a capacidade do crawler de baixar
%%páginas de conteúdo, uma vez que, antes de que qualquer conteúdo possa ser
%%obtido desses domínios, os arquivos \texttt{robots.txt} dele terá de ser
%%obtido.
%
%\subsubsection{Esgotamento da memória primária}
%
%Todas as estruturas de controle de nossa implementação são deixadas em
%memória primária. Em questão de horas isso começa a se tornar um
%problema, quando o S.O. inicia um processo de colocar algumas das
%páginas de memória de nosso sistema em  memória secundária. A medida que
%o tempo passa, vai se tornando cada vez mais difícil não ser afetado
%pela troca de páginas entre a memória principal e o \emph{swap}, fruto
%do consumo de memória de nossa aplicação.
%
%Uma forma de contornar o uso de memória seria fazer um uso mais
%agressivo de identificadores numéricos para as páginas. Isso será útil
%não somente para a contenção dos gastos com memória mas também quando
%estivermos realizando a indexação dos documentos.
%
%Encontrar mecanismos que nos permitam determinar a lista de URLs
%pendentes em um dominio além de rapidamente determinar se uma
%determinada URL já foi encontrada antes ou não sem depender tanto de
%memória primária é um problema não resolvido de nossa implementação.
%O uso de soluções como \emph{hashtables} distribuídas foi cogitado, mas
%não chegamos a implementá-lo.
%
%\subsubsection{Melhores implementações de listas de prioridades}
%
%A adoção da política de escalonamento LSF mostrou-se bastante benéfica à
%nossa implementação. Todavia, da forma que ela está implementada, ela
%sofre de uma severa limitação: uma vez que um domínio tenha entrado na
%lista de domínios ativos (Seção~\ref{sec:datastructures}), o valor do
%tamanho da sua fila de páginas pendentes não pode mais ser atualizado.
%Isso ocorre porque os algoritmos e estruturas disponíveis na STL, a
%bilbioteca padrão de tipos e algoritmos do C++, para lidar com listas de
%prioridades e \emph{heaps} não possuem métodos para decrementar ou
%incrementar um o valor atribuído a um ítem interno ao \emph{heap}. Essa
%limitação dificulta e limita também a implementação de políticas de
%escalonamento de domínios ou de páginas baseados na quantidade de
%\emph{links} que apontam para os mesmos.
%
%Um solução seria implementar \emph{heaps} binários e de Fibonacci do
%zero, de tal forma que esses possuíssem métodos para decrementar o valor
%associado a um ítem ou atualizá-lo dinamicamente. Foi cogitada a
%implementação uma estrutura similar a um \emph{heap} de Fibonacci mas
%com custos assintóticos inferiores, conhecida como \emph{relaxed heap},
%mas, por questões de tempo, essa implementação não foi
%possível~\cite{driscoll1988relaxed}.
%
%\subsubsection{Estrutura de diretórios}
%
%\subsubsection{UTF-8 vs Latin1}
%
%Considerando-se o fato de que o trabalho objetivava o uso de uma coleção
%de páginas da \emph{Web} brasileira e, por tanto, de conteúdo em língua
%portuguesa, as escolha de UTF-8 como mapa de caracteres utilizado após a
%normalização não parece mais um boa decisão. Considerando os problemas
%que UTF-8 causa no processo de parsing e normalização dos termos,
%percebe-se agora que o uso de Latin-1 seria um escolha mais sensata.
%Ainda mais considerando que, após a normalização de caso e de acentos
%nas palavras, tudo o que resta de útil é um conteúdo limitado de ASCII
%7-bits.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%% Bibliografia %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle {plain}
\bibliography{relatorio}


\end{document}

% vim:tw=72 fileencoding=utf-8 spelllang=pt spell syn=tex:
