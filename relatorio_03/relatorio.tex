\documentclass[10pt,twocolumn]{article}

\usepackage{graphicx,url}
\usepackage{subfigure}

\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[pdftex,colorlinks,%
                   citecolor=black,%
                   filecolor=black,%
                   linkcolor=black]{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Header  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Recuperação de Informação \\Trabalho Prático 3 -- Processamento
de Consulta}
\author{Tiago Alves Macambira \\ \texttt{tmacam@dcc.ufmg.br}}
\date{29 de julho de 2007}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Body  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Introdução}

O terceiro trabalho prático da disciplina de Recuperação de Informação
consiste no projeto e implementação de programas para
recuperação eficiente de grandes base de dados. Tal trabalho é uma
extensão do trabalho realizado no TP anterior, possuindo os seguintes
requisitos adicionais, como disposto no seu enunciado~\cite{tp3}:
\begin{itemize}
\item O sistema deve utilizar como base os dados coletados pelo
\emph{crawler} do TP1~\cite{tp1}.
\item Diferentemente do TP2, neste trabalho o aluno deverá processar e
\emph{rankear} consultas usando o modelo de espaço
vetorial~\cite{tp2}.
\item O sistema deve possuir uma interface \emph{Web} com a qual
o usuário interagirá com o mesmo.
\item Opcionalmente o sistema pode se utilizar de mecanismos de análise
de \emph{link} como forma de melhorar a qualidade da resposta gerada
pela solução.
\item O sistema deve ser escrito em C/C++. Bibliotecas extras somente
poderão ser utilizadas mediante autorização dos professores.
\end{itemize}

Nesse relatório, comentaremos sobre a resolução do nosso trabalho:
os desafios encontrados, as escolhas de projetos
adotadas e peculiaridades de nossa implementação. Além disso, comentamos
sobre o desempenho do mesmo, as estruturas de dados usadas bem como suas
complexidades.

Muitos dos pontos abordados nos trabalhos práticos anteriores são também
relevantes a esse trabalho prático. Por esse motivo e para tornar a
leitura desse relatório independente da leitura dos relatórios
anteriores, sempre que necessário, utilizaremos fragmentos dos mesmos.
Todavia, buscaremos não detalhar mais do que o necessário decisões
de projeto e peculiaridades dos TPs anteriores.

\vspace{0.5cm}
\rule[0.5mm]{0.9\linewidth}{0.5mm}

A implementação atual de nossa solução\
 se encontra operacional no
endereço \texttt{http://mimosa.grad.dcc.ufmg.br:8090}.

\rule[0.5mm]{0.9\linewidth}{0.5mm}


\section{Recuperação de documentos}\label{sec:retrieval}

Dado um conjunto de páginas \emph{Web} coletadas, para que seja possível
realizar buscas eficientemente nessa coleção é necessário, antes de mais
nada, indexar o seu conteúdo.

Uma vez que esse conteúdo tenha sido indexado, resta então o problema
de recuperar documentos da coleção que atendam uma ``necessidade de
informação'' ou consulta de um usuário. Ou seja, dada uma busca solicitada por um
usuário, representada através de um
conjunto de palavras-chaves (\emph{query}), localizar na colação
documentos que atendam a essa busca e, por alguma métrica, ordenar esses
documentos de forma a priorizar a exibição no topo dessa lista de
documentos mais relevantes. É importante frisar que
\emph{relevância} é uma prerrogativa do usuário. Tudo o que o sistema
pode fazer é se utilizar de heurísticas que priorizarão documentos que
\emph{devem}, sob a ótica do usuário, ser mais relevantes.

Uma das formas mais utilizadas atualmente por sistemas de buscas para
estimar a relevância de documentos dada uma consulta consiste no uso de
alguma medida de \emph{similaridade} entre os documentos e a consulta. A
intuição é que quanto maior a similaridade entre um dado documento e a
consulta, maior será a chance de que um humano considere esse documento
como relevante~\cite{moffat2006survey}.

Existem diferentes formas de determinar se um dado documento
atende (\emph{match}) uma busca  realizada pelo usuário e, associadas a
essas formas, diversos mecanismos de ordenação dos resultados. Cada uma
destas formas determina diferentes \emph{modelos de recuperação de
documentos}. No trabalho prático anterior implementou-se o modelo
booleano para recuperação de documentos. Neste, é solicitada a
implementação do modelo de espaço vetorial(\emph{Vector-Space Model}).

\subsection{Modelo de espaço vetorial}

No modelo de vetorial vetorial tanto um documento \(D\) como uma busca
\(Q\) são representados como vetores em um espaço \(t\)-dimensional,
onde \(t\) é o número de termos no vocabulário da coleção. A
similaridade entre o documento e a busca é dada pela ângulo entre esses
dois vetores~\cite{berthier1999modern}.
Esse ângulo é obtido através do cálculo do cosseno entre
os vetores de \(D\) e \(Q\), apresentada na equação~\ref{eq:sim}:

\begin{equation}
Sim(d_{j},q) = \frac{\sum_{i = 1}^{t}w_{i,j} w_{i,q}}{\sqrt{\sum_{i =
1}^{t}w_{i,j}^{2}} \sqrt{\sum_{j = 1}^{t}w_{i,q}^{2}}}
\label{eq:sim}
\end{equation}

Observe que \(w_{i,j}\), o peso do termo \(i\) no documento \(j\), é
dado pela equação~\ref{eq:tfidf}:


\begin{equation}
w_{i,j} = tf_{i,j}\times idf_{i}
\label{eq:tfidf}
\end{equation}

As fórmulas de \(tf_{i,j}\), que representa o quanto um termo é
discriminante em um dado documento, e de \(idf_{i}\), que representa o
quanto um dado termo é discriminatório sob o ponto de vista da coleção
como um todo, são apresentadas nas equações~\ref{eq:tf} e~\ref{eq:idf}:

\begin{equation}
tf_{i,j} = \frac{freq_{i,j}}{max freq_{j}}
\label{eq:tf}
\end{equation}

\begin{equation}
idf_{i} = log\frac{N}{n_{i}}
\label{eq:idf}
\end{equation}

É importante observar que, dada uma consulta, a norma do seu vetor
(\(|Q|\)), pode ser omitida dos cálculos para fins de ordenação de
resultados, uma vez que nesse caso ela será constante.

\subsection{Análise de links}

Além da similaridade, outras informações, tais como
proximidade entre palavras-chaves, taxa de atualização dos documentos,
 quantidade de \emph{links} de saída ou de \emph{links} de chegada em uma página
etc, podem ser utilizadas no processo de \emph{ranking} de documentos.

O uso de informações sobre sobre a ligação de uma página com as outras
(seus \emph{links}), ou \emph{link analysis}, tem se mostrado bastante
satisfatório, não somente no contexto de \emph{ranking} de documentos
mas também em políticas de escalonamento de
\emph{crawling}~\cite{baezayates2005crawling}. Todavia, a forma de se
utilizar essa informação e de aferir a esta um valor pode variar
bastante. Na literatura encontramos propostas tais como o HITS e o
PageRank que diferem na forma como que a informação de \emph{links} de
uma página é utilizada~\cite{page98pagerank}.

No modelo do \emph{PageRank}, a cada página é atribuído um valor, que
representa, de certa forma, a ``importância'' dessa página tal como ela
é vista pela relação de \emph{links} da \emph{Web}. De outra forma, o
\emph{PageRank} de uma página pode ser visto como medindo a chance de
que um surfista aleatório atinja uma determinada página.

Sejam \(u\) uma página, \(PR(u)\) o valor do \emph{PageRank} da página
\(u\), \(B_u\) o conjunto de páginas que possuem
\emph{links} para \(u\) e \(N_v\) o número de \emph{links} que
partem de uma página qualquer \(u\) para outras páginas na coleção ou na
\emph{Web}. Assim sendo, o \emph{PageRank} pode ser definido da
seguinte:

\begin{equation}
 PR(u) = (1-d) + d\sum_{v \in B_u}\frac{PR(v)}{N_v}
\label{eq:pagerank}
\end{equation}

Essa fórmula é recursiva mas o calculo do \emph{PageRank} para alguns
milhoes de páginas pode ser feito eficientemente em questão de horas
utilizando um algoritmo iterativo~\cite{brin1998google}.

%Nessa fórmula, \(d\)



\section{Problemas e desafios}

Nessa seção comentaremos sobre alguns dos problemas que podem ser
encontrados durante o processamento de consultas utilizando o modelo de
espaço vetorial e realizando análise de \emph{links} usando o PageRank.
de conteúdo HTML. Na
seção~\ref{sec:implementation} comentaremos como lidamos com esses
problemas na nossa implementação.

\subsection{Escolha do modelo de TF-IDF}

Além das fórmulas apresentadas na sessão~\ref{sec:retrieval}, existem
outras fórmulas para o cálculo de TF-IDF. Apesar da similaridade em
função de várias delas, é necessário observar que cada um possui
pequenas particularidades que podem torná-las mais atraentes alguns
determinados tipos de documentos ou coleções mas não para os demais.

\subsection{Valores pré-computados}

Tanto o modelo de espaço vetorial como o PageRank assumem que alguns
valores estão pré-computados ou que sua obtenção é fácil. Entre tais
valores podemos citar a norma ou o módulo de vetores de cada documento,
a freqüência do termo mais freqüente de cada documento e o valor de
PageRank de cada documento da coleção.

Obter esses valores em tempo de consulta é inviável. Alguns podem ser
calculados tão logo se termine a etapa de indexação dos
documentos, outros só poderão ser obtidos após o término da geração do
arquivo invertido. O momento e a forma de fazer isso pode ocasionar um
acréscimo de tempo de pré-processamento grande e, por isso, deve ser
dosado.

\subsection{Formas eficientes de calcular similaridade}

Apesar da diferença entre os modelos booleanos e o modelo de espaço
vetorial, a forma de se realizar processamento de consulta em ambos os
modelos possui algumas similaridades. Uma das mais óbvias é o fato de
que ambas utilizam as listas invertidas dos termos presentes na coleção
(e apenas estes) para a realização da consulta.

No entanto, no modelo vetorial, para cada lista invertida será
necessário acrescentar o peso que o termo daquela lista invertida
acrescentará no valor de similaridade de cada um dos documentos
existentes. Dependendo do tamanho da lista invertida, isso pode se
tornar caro, especialmente no caso de \emph{stopwords}, que possuem
listas invertidas bem maiores à média.

Diversas estratégias podem ser adotadas para tornar esse cálculo mais
eficiente ou até mesmo para evitar que cálculo desnecessário tenha de
ser feito. Uma das abordagens mais simples é limitar as consultas apenas
a buscas conjuntivas, limitando assim a quantidade de acumuladores que
têm de ser armazenados na memória a cada interação. Outras abordagens
possíveis são o uso de listas de impacto ou na reordenação das
listas invertidas por valores de freqüência de
documentos~\cite{moffat1999managing, persin1996frequency}.

\subsection{Extração de \emph{Links}}

Para a realização de \emph{link analysis} é necessário a obtenção da
lista de \emph{links} existentes em um dado documento \emph{Web}.
Apesar de ser uma atividade que já fora relizada durante o
\emph{crawling} dos documentos mas, caso essa informação não tenha sido
armazenada nesse momento, será necessário realizar novamente o
\emph{parsing} de todos os documentos. Sendo esse o caso, alguns dos
problemas que têm que ser enfrentados ao se realizar o \emph{parsing} de
documentos, muitos destes mencionados no TP1 e no TP2, terão de ser
enfrentados novamente:
\begin{itemize}
\item Normalização de mapas de caracteres
\item Descoberta de mapa de caracteres
\item \emph{Parsing} de páginas HTML
\item \emph{Parsing} de entidades
\item Internacionalização e \emph{tokenização} de texto
\end{itemize}

\subsection{Lidar com páginas-sumidouros}

Página-sumidouros (\emph{sinks}) são páginas que não apresentam
\emph{links} de saída. Tais páginas além de penalizar o valor de
PageRank de outras páginas também tornam a convergência do cálculo do
PageRank mais lenta~\cite{haveliwala99efficient, page98pagerank}.

Decidir como, se e quando esse tipo de página será tratado é uma decisão
que o implementador deve tomar.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROJETO %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{O projeto do processador de consultas}\label{sec:projeto}

Realizamos o projeto de nosso processador de consultas tendo em mente os desafios
mencionados na seção anterior e algumas decisões tomadas durante os
trabalhos práticos 1 e 2~\cite{tp1,tp2}. Algumas decisões de projeto tiveram que
ser tomadas tanto para contornar tais problemas como para ser capaz de
completar o trabalho em um prazo razoável.


\subsection{Premissas e decisões iniciais de projetos}

Como forma de limitar o volume de processamento realizado por consulta e
também como forma de melhorar os resultados das buscas submetidas ao
sistema, \textbf{processamos todas as buscas como buscas conjuntivas}.
Em nossos testes essa escolha apenas melhorou a qualidade das respostas.

\textbf{A obtenção das normas dos documentos (\(W_j\)) e dos
\(maxfreq_i\) será feita apenas após a construção do arquivo invertido}.
As normas apenas podem ser calculadas quando tivermos os valores de
\(n_i\), ou seja, número de documentos contendo um determinado termo
\(i\), e isso só é possível após o termino da criação do arquivo
invertido. Além disso, esse arquivo é consideravelmente menor do que a
coleção de documentos, e o processamento realizado para sua leitura bem
inferior ao do \emph{parsing} de documentos.

\textbf{A obtenção do grafo de vizinhança de páginas}, ou seja, da lista de
ligações entre documentos da nossa coleção, \textbf{ocorre após a coleta de
todas as páginas}. Caso fôssemos realizar o uma nova coleta de páginas,
repensaríamos esse ponto. Todavia, como o \emph{crawling} já foi feito, isso
realmente não representa muito uma decisão de projeto, mas apenas uma
forma viável de contornar essa limitação de nosso \emph{crawler}.


\textbf{Todas as URLs processadas pelo algoritmo de \emph{link analysis} são
normalizadas} e têm seus campos \emph{query} e fragmento removidos. Os
mecanismos de normalização são os mesmos descritos nos TPs anteriores,
ou seja, baseados nas especificações da RFC 3986~\cite{rfc3986}.

Para contornar o volume de memória necessário para o armazenamento e
processamento do grafo de vizinhança \textbf{utilizaremos não as URLs em si, mas
suas ``impressões digitais''} (\emph{fingerpring}, geradas pelo uso da
função ou \emph{hash} FNV-1~\cite{fnv1}. Observamos essa função \textbf{não
apresenta colisões mesmo para o conjunto de aproximadamente 9 milhões de
URLs descobertas pelo nosso \emph{crawler}}.

\textbf{Removemos \emph{dangling links}}, ou seja, \emph{links} para
páginas-sumidouros em dois momentos:
\begin{enumerate}
\item durante a re-leitura da base, quando fazemos a extração das URLs
dos documentos,
\item durante o cálculo do PageRank, quando removemos interativamente
\emph{links} para páginas que se mostraram como páginas sem ligações de 
saída (\emph{outlinks}).
\end{enumerate}

Devido ao tamanho de nossa base e às características de ligação dos
nossos documentos, \textbf{o processamento do \emph{PageRank} envolverá
memória secundária}. Não como armazenento secundário, mas simplesmente
\textbf{pelo fato de que o grafo não cabe na memória primária}. Mais a
frente discutiremos isso e o desempenho dessa etapa.

\textbf{Assumimos que várias das nossas estruturas de controle caberão na
memória}. Entre tais estruturas temos:
\begin{itemize}
\item o vocabulário,
\item as normas dos documentos,
\item a freqüência do termo mais freqüente de cada documento,
\item o valor de PageRank de cada documento,
\item o índice da base de títulos de URLs dos documentos.
\end{itemize}

\textbf{A interface entre o nosso sistema e o usuário se dá através de um
servidor HTTP próprio, single-threaded e blocante}. Esse servidor foi
feito especificamente para esse projeto e, a despeito de suas
limitações, atende aos propósitos da disciplina.

\subsection{Componentes}


Os componentes da no nosso \emph{processador de consultas} são:
\begin{description}
\item[armazém de dados:] esse sistema foi decomposto em dois, um sendo 
uma fina camada sobre o sistema de arquivos que
permite organizar os dados coletados/indexados e outra sendo um base de
dados indexada para acesso sequencial (ISAM), que permite acesso
sequencial muito mais rápido do que o sistema anterior.
\item[mknorms:] realiza a leitura e processamento do arquivo invertido
para geração das normas dos documentos e de outros dados que são
utilizados durante o processamento de consultas.
\item[mkmeta:] realiza o \emph{parsing} dos documentos e extrai desses
seus títulos e suas URLs, para uso posterior.
\item[mkprepr:] realiza o \emph{parsing} dos documentos e extrai destes
a lista de ( \emph{fingerprints} de) URLs de saída (\emph{outlinks}) de
cada página. A saída desse componente é utilizada como entrada para o
programa que calcula o PageRank das páginas.
\item[myserver:] nossa interface com o processador de consultas, ou
seja, nosso servidor HTTP. O processador de consultas em si encontra-se
como um componente usado em por esse servidor.
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPLEMENTAÇÃO %%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementação}\label{sec:implementation}

\subsection{Decisões de implementações}

Todo o nosso código foi escrito em C++
e utiliza a biblioteca de gabaritos padrões (STL) dessa
linguagem~\cite{stroustrup97}.
Não foi utilizada nenhuma nova liblioteca em comparação às já utilizadas
no TP1.

Além disso, nosso código faz uso extensivo de uma técnica para controle
de recursos (como memória, \emph{locks} etc)  muito comum em C++,
denominada RAII (\emph{Resource Acquisition Is Initialization}).
Diversas partes de nosso código possuem código de teste (\emph{Unit
Testing}).

Sempre que possível, delegamos o processo de leitura de
arquivos ao S.O. através do uso de \texttt{mmaps}.

\subsection{Estruturas de armazenamento}\label{sec:storage}

A forma como que os dados são gravados e recuperados do disco é
tão importante quanto os algoritmos e estruturas de dados utilizados
pelo nosso indexador. As estruturas de armazenamento podem ditar ou
limitar a efetividade dos algoritmos utilizados.

Em nossa solução, a exceção dos arquivos coletados pelo crawler, todas
as outras estrutuas (vocabulário, arquivo invertido, metadados sobre as
páginas, valores de PageRank) possuem uma estrutura
de armazenamento similar (ISAM) e composta, de maneira geral, de dois
elementos:
\begin{description}

\item[Arquivo-cabeçalho] Uma estrutura que permite acesso rápido a
informações sobre um determinado termo ou documento.  Tais informações
consistem, entre outras coisas, na localização (\emph{offset} ou
ponteiro) do conteúdo de interesse no arquivo(s) de dados . \footnote{
Na prática, esse arquivo nada mais é do que o \emph{dump} de um
\emph{array}, o que, ao utilizarmos técnicas como \texttt{mmap}, permite
o aceso quase que em tempo constante à informação desejada.}

\item[Arquivo(s) de dados] Nesse arquivo, termos do vocabulário ou listas
invertidas propriamente ditas são armazenadas concatenadas. A
localização de um item em um arquivo de dados depende da leitura do
arquivo-cabeçalho correspondente.
\end{description}

No caso do vocabulário, o arquivo de dados consiste na concatenação das
palavras do vocabulário em ordem ascendente de seus \(t_{id}\).

As listas invertidas podem, quando concatenadas, podem vir a atingir um tamanho
superior ao tamanho máximo de um arquivo no SO utilizador. Para evitar
problemas, os dados das listas invertidas e de outras estruturas de
dados grandes, são distribuídos entre vários
arquivos de dados, cada um de aproximadamente 256~MB. A localização de
uma lista invertida relativa a um termo em um destes arquivo de dados
bem como a informação de em qual arquivo de dados essa lista se encontra
é guardada no arquivo-cabeçalho do arquivo invertido. De maneira similar
é possível encontrar, por exemplo, a lista de \emph{outlinks} de uma
dada página.

\subsection{Estruturas de dados}\label{sec:datastructures}

Nossa implementação usa as seguintes estruturas de
dados~\cite{cormen-algorithms}:

\begin{itemize}

\item No processamento de consulta, também se utiliza uma
\emph{hashtable} (termo\(\rightarrow t_{id}\)) para armazenar o
vocabulário.  Busca e inserção nessa estrutura possui uma complexidade
de tempo médio de \(O\left(1 \right)\). Uma \emph{hashtable} também é
utilizado no processo de cálculo do PageRank (nossos identificadores não
são seqüenciais) e, na coleta de página, para verificação se cada as
novas páginas descobertas pertencem ao conjunto de páginas que foram
efetivamente coletadas.

\item Nos vários pontos em que é necessário realizar ordenação dos dados
(geração do resultado do modelo de espaço vetorial, ordenação do
resultado do casamento do modelo vetorial com o PageRank, etc) utilizamos
o algoritmo \emph{introsort}\footnote{ D. R. Musser,
"Introspective Sorting and Selection Algorithms", Software Practice and
Experience 27(8):983, 1997.  } cuja complexidade de pior caso é
\(O\left(n \log n\right)\)

\item Na processamento da consulta no modelo de espaço vetorial a busca
conjuntiva é feita pela interseção duas-a-duas das listas de documentos
encontrados. Esse algoritmo possui custo liner.

\end{itemize}


\subsection{Dificuldades}

Nessa sub-seção discutiremos alguns dos problemas enfrentados durante a
elaboração e teste da nossa implementação.

\subsubsection{Obtenção das normas, etc}

Como já mencinado anteriormente, um dos problemas que enfrentamos foi o
cálculo das normas dos documentos e de outros valores que são utilizados
pelo processador de consulta. Boa parte desse trabalho poderia ter sido
feito durante o TP1, que tratava da construção do arquivo invertido.

\subsubsection{Número de Links}

Durante a elaboração do código responsável pelo cálculo do PageRank nos
defrontamos com o seguinte problema: nosso arquivo contendo a lista de
\emph{outlinks}, essencialmente uma representação do grafo de ligação
entre páginas como uma lista de adjacências, estava atingindo a marca de
aprox. 2,5~GB. Esse arquivo possuía apenas as \emph{fingerprints} de
cada URL, cada um ocupando 8 bytes. Mesmo levando em considerando o
número de documentos baixados pelo \emph{crawler} e mesmo o número de
URLs descobertas pelo mesmo ( aprox. 1,5 milhões e 9 milhões) e número
médio de links por páginas Web como 11~\cite{page98pagerank}, ainda
assim verifica-se que o volume de 2,5 GB está muito longe do esperado.


\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.5\textwidth]{plot_linkranking}
\end{center}
\caption{Distribuição de links por páginas. (Log-Normal)}
\label{fig:linkspp}
\end{figure}

Para determinar se existia alguma problema em nossa solução ou se
existia alguma anomalia dos dados coletados, verificamos como se dava a
distribuição de \emph{links} por página nos documentos de nossa coleção.
O resultado dessa análise pode ser visto na Figura~\ref{fig:linkspp}.
Nessa figura, disposta em escala log-normal, pode-se observar que existe
uma significativa proporção de nós que possui mais de 200 links de
saída. Isso explica o motivo pelo qual nosso arquivo de \emph{outlinks}
tornou-se tão grande.

Por amostragem determinou-se que boa parte desses links correspondiam a
páginas do sítio \texttt{letras.terra.com.br} e seus sub-domínios. Em
conjunto, existem 1019681 páginas em nossa base cujas URLs possuem o
fragmento de texto \texttt{letras.terra}. Isso corresponde a quase dois
terços de nossa base, o que explica ainda mais o que aconteceu: baixamos
muitas página desses sítio e esse sítio e seus subdomíno estão
fortemente ligados entre si.

Ao invés de retirar esses documentos da base para tornar o cálculo do
PageRank viável utilizando somente memória primária, decidimos por
realizar o cálculo do PageRank de tal forma que, a cada iteração, os
valores são re-lidos do disco.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Testes e Resultados}

\subsection{Ambiente experimental}

Todos os testes de nossa implementação foram realizados em um micro do
laboratório de graduação do DCC/UFMG, rodando como S.O. Linux Gentoo
1.12.9, kernel 2.6.19-gentoo-r5, dotado de uma CPU Pentium 4 de
3.00~MHz, 1~GB de memória RAM e 1~GB de \emph{swap}.

\subsection{Resultados}

\begin{table}[htbp]
\centering
\begin{tabular}{|c|r|} \hline
Número de documentos			& 1.572.822		\\\hline
Volume do vocabulário			&	      29.948 KB \\\hline
Volume da base de dados 		&   			\\
          compactado 			&            43.460.994 KB \\
          compactado 			&   	  13.977.374 KB \\\hline
Volume das runs 			&	   7.180.092 KB \\\hline
Volume do índice  			&			\\
  sem compressão  			&	   5.764.740 KB \\
  com compressão  			&	   1.547.700 KB \\\hline
Tempo para geração das normas 		&	    7m59.778s	\\ \hline
Tempo para extração de \# links		&		4h12m02s \\ \hline
PageRank				& 		\\
Tempo para convergência& 02h51m21s	\\
Valor Residual (norma \(L_1\)		& 	0.0500221 \\
Nº de Iterações até covergência		& 	89	\\ \hline
Total de \emph{links} (arestas) válidas & 297.401.139	\\ \hline
  sem compressão 			&	   9m43.120s \\
  com compressão  			&	   7m12.012s \\\hline
Velocidade de indexação			&		\\
  doc/s		  			& 20,568		\\
  KB/s					& 562			\\\hline
Número de Termos			&     2.311.837	\\\hline
\hline
\end{tabular}
\caption{Dados sobre as coletas}
\label{tab:sumario}
\end{table}


A Tabela~\ref{tab:sumario} apresenta um sumário dos valores de
desempenho e de outras informações pertinentes sobre esse TP.

Observe que o tempo de geração de normas engloba também o 
tempo para a extração e geração de outros valores gerados pelo
utilitário \texttt{mknorms}, como \(maxfreq_j\) e \(n_i\). Observe ainda
que o total de arestas válidas corresponde apenas a ligações entre
páginas baixadas na coleção para páginas com URLs que correspondem a
páginas baixadas na coleção.

\subsubsection{Convergência do PageRank}

Mesmo realizando, a cada iteração, a leitura dos arquivos com as listar
invertidas, consideramos que nossa implementação do PageRank executou
com relativa velocidade. Cada iteração demora em média 115 segundos.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.5\textwidth]{plot_pagerankconverge}
\end{center}
\caption{Convergência do PageRank (escala Log-Normal)}
\label{fig:converge}
\end{figure}

Na Figura~\ref{fig:converge} podemos ver o gráfico de convergência do
PageRank, apresentado em escala Log-Normal. O valor da norma \(L_1\) é
calculado para as diferências das estimativas de PageRank encontradas em
duas execuções consecutivas. A fórmula para o cálculo da norma \(L_1\) é
apresentada na equação~\ref{eq:l1}. Como pode ser visto na figura, até
aproximadamente a interação 70 a curva apresenta o decaimento
exponencial -- evidenciado pela reta no gráfico em escala
log-normal. Após isso, o decaimento é consideravelmente mais lento,
indicando que para obter uma maior precisão (ou seja, um valor menor de
norma \(L_1\)), um número de iterações será bemm maior, não justificando
seu custo.

\begin{equation}
L_1(X) = \sum_{i=0}^{n}|x_i|
\label{fig:li}
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Conclusões}
%
%Nesse relatório descrevemos a implementação do \emph{crawler}
%especificado no TP1 da disciplina~\cite{tp1}, bem como os problemas
%enfrentados e as decisões de projeto tomadas.
%
%Pudemos observar como a política de escalonamento possui um papel
%fundamental no desempenho do coletor. Em nossos experimentos, o
%uso da política ``\emph{largest site first}'' mostrou-se capaz de, em
%50\% do tempo necessário, baixar 150\% do conteúdo desejado para um
%único dia de coleta. Além disso, mostro-se quase duas vezes mais eficiente do que a
%política ``\emph{breadth-first}''. Esses resultados, embora bastante
%interessantes, estão longe de ser novidades, sendo bastante estudados na
%literatura~\cite{baezayates2005crawling}.
%
%
%Finalmente, foi também possível observar como que, a despeito de ser
%aparentemente um problema simples, a elaboração de um \emph{crawler}
%industrial é complicada.
%
%\subsection{Problemas encontrados e Melhorias
%futuras}\label{sec:problemas}
%
%Durante a implementação do \emph{crawler}, diversos problema foram
%observados mas, devido a problema relativos ao tempo de entrega do
%trabalho, tiveram que ser deixados de lado. Todavia, esses problemas
%merecem menção, uma vez que constituem limitações da implementação e
%que, em TP's futuros podem se revelar como problemas cuja solução será
%necessária.
%
%\subsubsection{Armadilhas para \emph{crawlers}}\label{dif:armadilhas}
%
%Um dos problemas de se lidar com a coleta de conteúdo dinâmico é que
%sítios com esse tipo de página pode conter o que se chama e ``buracos
%negros'', ou seja, é possível que o coletor possa se perder ou passar
%mais tempo do que o necessário tentando coletar uma
%infinidade de páginas geradas dinamicamente. Ignorar o conteúdo do
%componente \emph{query} de URLs é uma forma de tentar contornar esse
%problema mas está longe de ser uma solução eficaz, uma vez que existem
%formas de se apresentar conteúdo dinâmico sem que esse envolva o uso de
%URLs com componentes \emph{query}.
%
%Uma outra forma de gerar conteúdo ou URLs dinâmicas é através do
%componente \emph{domainname} de URLs. Essa forma é particularmente
%nociva à nossa implementação, sobretudo na que usa a política de
%escalonamento BF. Isso deve-se a diversos fatores:
%\begin{itemize}
%\item Quando uma página em um domínio desses é encontrada pela primeira
%vez, acaba-se descobrindo por tabela diversas páginas em diversos
%domínios dinâmicos pertencentes ao mesmo sítio;
%\item uma vez que cada novo domínio gerado
%acarreta a criação de uma instância de gerente de domínio em nossa
%implementação, ao visitar apenas uma única página num sítio desses
%diversas instâncias de gerentes de domínios terão de ser geradas
%instantaneamente,
%\item finalmente,  como nessa política os domínios são visitados por ordem de
%descoberta, o inicio da coleta do primeiro domínio dinâmico de um sítio
%desses acarretará diversas requisições DNS, uma para cada um dos outros
%domínios dinâmicos encontrados em seqüência.
%\end{itemize}
%
%Nessas circunstâncias o desempenho do coletor cai para patamares
%baixíssimos, como pode-se observam em alguns vales na
%Figura~\ref{fig:bf}. A política de escalonamento LSF contorna isso de
%maneira elegante, mas até que ponto? Se tivessemos deixado o coletor com
%LSF rodando por mais tempo teríamos encontrado esse tipo de
%comportamento?
%
%%Buracos negros - nossa abordagem de escalonamento foi baseada em domínios. Isso
%%permitia não somente controlar o tempo mínimo de intervalo entre acessos
%%consecutívos a um mesmo domínio, limitar o uso de memória (já que nao era mais
%%necesári armazenar o nome de domínio em cada URL pertencente a ele) e armazenar
%%a lista de URLs pendentes. Todavia, para a nossa implementação, sítios que
%%fazem uso extensivo de sub-domínios para sua organização são torna um problema.
%%Isso deve-se ao fato de que, custumeiramente, a obtenção e \emph{parsing} de
%%uma unica página de um sítio destes acarreta na ``descoberta'' de vários
%%sub-domínos desse sítio, o que por sua vez acarreta no acréscimo de cada um
%%destes na lista de domínios conhecidos e de domínios pendentes. Além desses
%%domínios serem costumeriramente rasos, ou seja, sem muitos links para
%%documentos internos, a sua adição limitará a capacidade do crawler de baixar
%%páginas de conteúdo, uma vez que, antes de que qualquer conteúdo possa ser
%%obtido desses domínios, os arquivos \texttt{robots.txt} dele terá de ser
%%obtido.
%
%\subsubsection{Esgotamento da memória primária}
%
%Todas as estruturas de controle de nossa implementação são deixadas em
%memória primária. Em questão de horas isso começa a se tornar um
%problema, quando o S.O. inicia um processo de colocar algumas das
%páginas de memória de nosso sistema em  memória secundária. A medida que
%o tempo passa, vai se tornando cada vez mais difícil não ser afetado
%pela troca de páginas entre a memória principal e o \emph{swap}, fruto
%do consumo de memória de nossa aplicação.
%
%Uma forma de contornar o uso de memória seria fazer um uso mais
%agressivo de identificadores numéricos para as páginas. Isso será útil
%não somente para a contenção dos gastos com memória mas também quando
%estivermos realizando a indexação dos documentos.
%
%Encontrar mecanismos que nos permitam determinar a lista de URLs
%pendentes em um dominio além de rapidamente determinar se uma
%determinada URL já foi encontrada antes ou não sem depender tanto de
%memória primária é um problema não resolvido de nossa implementação.
%O uso de soluções como \emph{hashtables} distribuídas foi cogitado, mas
%não chegamos a implementá-lo.
%
%\subsubsection{Melhores implementações de listas de prioridades}
%
%A adoção da política de escalonamento LSF mostrou-se bastante benéfica à
%nossa implementação. Todavia, da forma que ela está implementada, ela
%sofre de uma severa limitação: uma vez que um domínio tenha entrado na
%lista de domínios ativos (Seção~\ref{sec:datastructures}), o valor do
%tamanho da sua fila de páginas pendentes não pode mais ser atualizado.
%Isso ocorre porque os algoritmos e estruturas disponíveis na STL, a
%bilbioteca padrão de tipos e algoritmos do C++, para lidar com listas de
%prioridades e \emph{heaps} não possuem métodos para decrementar ou
%incrementar um o valor atribuído a um ítem interno ao \emph{heap}. Essa
%limitação dificulta e limita também a implementação de políticas de
%escalonamento de domínios ou de páginas baseados na quantidade de
%\emph{links} que apontam para os mesmos.
%
%Um solução seria implementar \emph{heaps} binários e de Fibonacci do
%zero, de tal forma que esses possuíssem métodos para decrementar o valor
%associado a um ítem ou atualizá-lo dinamicamente. Foi cogitada a
%implementação uma estrutura similar a um \emph{heap} de Fibonacci mas
%com custos assintóticos inferiores, conhecida como \emph{relaxed heap},
%mas, por questões de tempo, essa implementação não foi
%possível~\cite{driscoll1988relaxed}.
%
%\subsubsection{Estrutura de diretórios}
%
%\subsubsection{UTF-8 vs Latin1}
%
%Considerando-se o fato de que o trabalho objetivava o uso de uma coleção
%de páginas da \emph{Web} brasileira e, por tanto, de conteúdo em língua
%portuguesa, as escolha de UTF-8 como mapa de caracteres utilizado após a
%normalização não parece mais um boa decisão. Considerando os problemas
%que UTF-8 causa no processo de parsing e normalização dos termos,
%percebe-se agora que o uso de Latin-1 seria um escolha mais sensata.
%Ainda mais considerando que, após a normalização de caso e de acentos
%nas palavras, tudo o que resta de útil é um conteúdo limitado de ASCII
%7-bits.
%
%

\section{Agradecimentos}

Esse trabalho não poderia ser entregue sem a ajuda de Hélio Almeida e
sem a colaboração de  Anísio Lacerda, que forneceu o código das funções
de hash.


%%%%%%%%%%%%%%%%%%%%%%%%%% Bibliografia %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle {plain}
\bibliography{relatorio}


\end{document}

% vim:tw=72 fileencoding=utf-8 spelllang=pt spell syn=tex:
