\documentclass[10pt,twocolumn]{article}

\usepackage{graphicx,url}
\usepackage{subfigure}

\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage{times}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Header  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Trabalho prático 1}
\author{Tiago Alves Macambira \\ \texttt{tmacam@dcc.ufmgbr}}
\date{4 de maio de 2007}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Body  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Introdução}

O primeiro trabalho prático da disciplina de Recuperação de Informação
consiste na elaboração de um \emph{crawler} industrial para paginas Web.
Esse \emph{crawler} deveria atender vários requisitos, como disposto no
enunciado do trabalho~\cite{tp1}, dentre os quais podemos citar:
\begin{itemize}
\item O sistema deveria ser capaz de coletar 1~milhão de URLs por dia
utilizando um servidor padrão,
\item O coletor deve respeitar as ``regras de etiqueta para
\emph{crawling} de páginas \emph{Web}''.
\item A velocidade de coleta de deve se manter constante
\item O coletor deve ser escrito em C/C++. Bibliotecas extras somente
poderão ser utilizadas mediante autorização dos professores.
\end{itemize}

\subsection{Terminologia}

No restante desse relatório serão utilizados alguns termos de uso
não-comum. Afim de evitar possíveis problemas de interpretação,
disporemos seus significados aqui.

\begin{description}
\item[URL pendente:] Uma URL que foi descoberta mas não foi requisitada aos seu
servidor.
\item[Domínio:] Um domínio nada mais é do que um nome de domínio
(DNS \emph{domainname}) atrelado a um
endereço IP. Essa mesma terminologia é costumeiramente utilizada quando
tratamos de DNS~\cite{rfc1034}.
\item[Parsing:] Leitura de uma página em que é realizada a interpretação do
conteúdo nela presente, geralmente consistindo de marcação de conteúdo HTML e,
no caso de arquivos \texttt{robots.txt}, de conteúdo com sintaxe de arquivos
\texttt{robots.txt}.
\item[Sítio:] Uma localidade na internet, geralmente representada um um domínio
e um conjunto de sub-domínios. Apesar de ter o mesmo significado do termo
inglês \emph{site}, preferimos utilizar o nome em português. É importante
salientar que um sítio pode ser composto por mais de um domínio.
\item[Tag] Elemento de marcação utilizado em documentos HTML e XML.
\end{description}

\section{\emph{Crawling} de páginas web}

\emph{Web crawlers}, também conhecidos como \emph{Web spinders} ou
\emph{Web robots} são programas ou sistemas automatizados que percorrem
a \emph{Web} de maneira automatizada. Esses programas estão geralmente
associados a sistemas de máquinas de busca e, nesse contexto, têm como
objetivo coletar páginas da WWW para serem posteriormente indexadas.

Durante o processo de coleta de páginas, os \emph{crawlers} vão
descobrindo novas URLs que serão enviadas a um gerente e futuramente
escalonadas para obtenção.

\subsection{Componentes de um crawler}\label{sec:components}

De forma resumida, um \emph{crawler} para a \emph{Web} possui os
seguintes componentes:

\begin{description}

\item[Coletores de páginas:] São os elementos que efetivamente realizam
a coleta de página. Sistemas de \emph{crawling} para a \emph{Web}
geralmente empregam vários coletores em paralelo para aumentar a sua
capacidade de coleta de páginas. Esses coletores podem ser responsáveis
apenas pela realização das requisições HTTP como podem ser também
responsáveis pela realização de algum tipo de pre-processamento das
páginas.

\item[Extrator de links:] É o elemento que realiza o \emph{parsing} dos
documentos coletados, extraindo \emph{links} destes. Esse componente
também é responsável por normalizar as URLs encontradas no documento bem
como por resolver URLs relativas presentes no mesmo. Apesar de ter um
papel definido, esse elemento pode se encontrar contido ou associado a
elementos coletores de páginas.

\item[Escalonador de Requisições:] As URLs extraídas pelo elemento
anterior são reportadas a esse componente. É papel do
escalonador gerenciar e distribuir URLs a serem vasculhadas pelos
coletores. Esse processo de seleção deve garantir que os sítios
visitados não sejam soterrados por requisições, que essas requisições
respeitam as politicas indicativas para coleta dispostas nos sítio e que
a coleção de paginas baixadas tenha uma amostra relevante das páginas na
internet.

\item[Armazém de Páginas] As páginas coletadas podem não sofrer toda a
análise necessária para os propósitos de uma máquina de busca. Assim
sendo, será necessário guardar essas páginas para posterior
processamento. Esse componente de um sistema de \emph{web crawling} é o
que fica responsável por essa tarefa. Observe que a existência desse
componente não é estritamente necessário para um \emph{crawler} mas é
fundamental para uma máquina de busca, uma vez que a indexação pode
ocorrer desassociada da extração de links.
\end{description}

\subsection{Problemas e desafios}

Apesar da aparente simplicidade, a realização de coleta de páginas na
WWW apresenta alguns desafios particulares que, se não forem tratados,
podem comprometer a capacidade do coletor ou até mesmo inviabilizar o
resultado de sua coleta. 

Nessa seção comentaremos sobre alguns desses problemas. Na
seção~\ref{sec:implementation} comentaremos como lidamos com esses
problemas na nossa implementação.

\subsubsection{Resolução de nomes de domínios}\label{prob:dns}
Uma vez de posse de uma lista de URLs a serem coletadas, cada coletor
deve então resolver cada nome do domínio existente nas URLs da sua lista
para um endereço IP, para que ele possa então entrar em contato com o
servidor da página desejada e solicitá-la. Dependendo da velocidade de
coleta do \emph{crawler} e da dispersão dos nomes de domínios em suas
listas de URLs, cada coletor passará um tempo considerável no processo de
resolução de nome.

A utilização de \emph{caches} pode aliviar esse problema, mas é
importante ter em mente que o sistema de resolução de nomes na Internet,
o DNS, permite que nomes de domínio tenham uma valida tão ínfima quanto
o tempo de duração da própria requisição, o que de encontro com a
utilização de \emph{caches}.

\subsubsection{Normalização de mapas de
caracteres}\label{prob:charmapnorm}

As diferentes línguas utilizadas pelo homem usam distintos conjuntos de
símbolos para sua representação e, para vários desses conjunto de
símbolos, mais de uma forma de representação binária existe. Esse fato
torna complicado a troca e a interpretação de texto em línguas diferentes
e mesmo para documentos escritos na mesma língua. Para a língua inglesa,
por exemplo, que utiliza um conjunto de caracteres bem limitado, existem
dois conjuntos de mapas de caracteres históricos diferentes e
incompatíveis, ASCII e EBCDIC. Se contarmos os mapas de caracteres e
formas de codificação diferentes existentes para as várias línguas
humanas, esse problema torna-se bem mais complexo.
Mesmo a existência de conjuntos ``universais'' como UTF-8, UTF-16 e
UTF-32 não resolvem esse problema, pois nem todos os documentos na
Internet se encontram codificados nesses mapas de caracteres.

É salutar observar que a capacidade de interpretar corretamente
documentos escritos em mapas de caracteres distintos e de recodificá-los
para um mapa de caractere comum  repercute não somente na capacidade de
realizar \emph{parsing} de documentos, mas também na capacidade de
indexação e extração de \emph{links} do mesmo, uma vez a forma de
representar caracteres não-ASCII irá afetar diretamente a capacidade de
normalizar o vocabulário e de normalizar URLs.

\subsubsection{Descoberta de mapa de
caracteres}\label{prob:charmapdetection}

Associado ao problema de normalizar os documentos em um único mapa de
caracteres está o problema de, dado um documento, descobrir em qual mapa
de caracteres o mesmo se encontra para que se possa então realizar a
tradução. Informações sobre o mapa de caractere usado por uma página web
podem ser fornecidas de diversas formas:
\begin{itemize}
\item por cabeçalhos da requisição HTTP~\cite{rfc2616},
 o protocolo de aplicação utilizado na \emph{Web},
\item pelo prólogo de um documento XML~\cite{bray2006xml},
\item através de uma \emph{tag} \texttt{Meta} em um documento
(x)HTML~\cite{html4tr}.
\end{itemize}

Entretanto, os padrões acima apresentam sobreposições, colocando o
desenvolvedor em situações de impasse. Por exemplo, quando mais de um
padrão fornece informações conflitantes, qual utilizar?  Quando nenhum
mecanismo acima informa nada sobre o mapa de caracteres usado, qual
padrão ou qual regra usar por omissão?

%http://feedparser.org/docs/character-encoding.html
%    COmo é feito no XML
%        Mas ele tem UTF8 como default
%        http://www.w3.org/TR/REC-xml/\#sec-guessing-no-ext-info
%    HTTP defaults to ISO 8859-1 (Latin 1)
%    O que a RFC 3023 diz? HTTP  Headers -> XML Dec. -> UTF-8
%
% Windows-1252 (http://en.wikipedia.org/wiki/ISO\_8859-1)

Uma alternativa plausível é realizar o mesmo processo utilizado por
navegadores \emph{Web}, que é uma extensão do modelo proposto pela RFC
3023~\cite{rfc3023}. Esse processo consiste em seguir os passos abaixo,
parando no primeiro que tenha suas restrições atendidas:
\begin{enumerate}
\item Usar as informações dos cabeçalhos HTTP, sempre que possível e
sempre que estes estejam disponíveis.
\item  Se os primeiros 2--4 bytes forem macadores de ordem de bytes
(BOM, de \emph{Byte Order Mark}) XML ou UTF, a codificação
correspondente será utilizada.
\item Se o documento iniciar com um prólogo XML legível e que informe a
codificação do documento, esse será utilizado.
\item Se o documento conter a tag HTML \texttt{meta}, como em \texttt{<meta
http-equiv="Content-Type" ...> }, o mapa de caracteres declarado nessa
tag será usado.
\item Finalmente, como última opção, existe a abordagem de detecção de
mapas de caracteres por heurísticas compostas~\cite{mozillaiuc}
\end{enumerate}

\subsubsection{\emph{Parsing} de páginas}

Apesar de ser um problema de relevância maior no processo de indexação
de documentos, o \emph{parsing} de documentos HTML também é um problema
a ser enfrentado por um extrator de \emph{links}.

Os documentos HTML existentes na \emph{Web} atual não totalmente válidos
e/ou bem formados se confrontados com os padrões existentes e publicados
pela W3C para HTML e XHTML~\cite{html4tr, bray2006xml}, sendo comumente
uma mistura dos dois padrões. Uma busca por URLs de maneira simples como
através do uso de expressões regulares, se eficiente, mostra-se ingênua
quando deparada com documentos HTMLs mais complexos. Não somente uma
abordagem dessas pode ignorar seções como comentários HTML/XML, mas
também ignorar \emph{tags} espciais do HTML tais como a \texttt{SCRIPT},
\texttt{TEXTAREA} e \texttt{STYLE}, que, se presentes, podem conter
conteúdo que não é HTML e que não deve ser recuperado por um extrator de
URLS.

\subsubsection{Normalização de URLs}\label{prob:urlnorm}

Outro aspecto importante de um \emph{crawler} é a sua capacidade de
normalizar URLs. A normalização a vários propósitos mas, sobretudo, para
evitar a duplicação de esforços na obtenção de páginas que são
essencialmente a mesma. Existem várias técnicas que podem ser utilizadas
para o processo de normalização de URLs, cada uma delas se aplicando a
cada um dos 5 componentes distindos de uma URL: esquema, autoridade,
\emph{path}, \emph{query} e fragmento. Várias dessas técnicas são
tratadas na RFC 3986~\cite{rfc3986}.

%\subsubsection{Obtenção de páginas com conteúdo dinâmico}
%Crawling de páginas dinâmicas. O que constituem páginas dinâmicas.
%Por que baixá-las ou evitá-las.

\subsubsection{Boas-práticas para \emph{crawling} de
páginas}\label{prob:robots}

A atividade de coleta de um sítio por um \emph{crawler} não pode ser por
demais onerosa ao primeiro. Caso contrário, a coleta poderá inviabilizar
o funcionamento normal de um sítio e haverá motivos para que os
administradores desse sítio bloqueiem acessos futuros dos coletores a
ele. Em outra situações, os administradores de um sítio podem determinar
que eles não desejam ter parte ou todo o seu sítio indexado ou visitado
pelo \emph{crawler}.

Para solucionar esse problema de maneira ``cordial'', foram
desenvolvidos dois mecanismos que indicam aos coletores de páginas quais
restrições os administradores de um domínio aplicam às suas páginas. O
primeiro e mais antigo consiste em colocar um arquivo, entitulado
\texttt{robots.txt} na raiz dos dominínios indicando quais páginas podem
ou não ser coletadas~\cite{robotstxt}. O segundo consistem em colocar
uma \emph{tag} \texttt{meta} especial no cabeçalho do arquivo HTML que
informa se aquela página pode ou não ser indexada e se pode ou não ser
seguida, ou seja, se os links contidos naquela página podem ser
acrescentados à lista de \emph{links} de um
\emph{crawler}~\cite{robotsmeta}.

Além disso, um dos requisitos desse trabalho era que nenhuma requisição
a um dado servidor fosse realizada em um intervalo inferior a 30 segundos.

%\subsubsection{Escalonamento de URLs e gerência de URLs visitadas}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROJETO %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{O projeto do \emph{CrawlerBeast}}

Realizamos o projeto de nosso \emph{crawler} tendo em mente os desafios
mencionados na seção anterior. Algumas decisões de projeto tiveram que
ser tomadas tanto para contornar tais problemas como para ser capaz de
completar o trabalho no prazo estipulado.

\subsection{Premissas e decisões iniciais de projetos}

Uma das primeira decisões de projeto foi a forma como lidaríamos com o
requisito de intervalo mínimo de 30 segundos. Optamos por considerar que
cada \textbf{\emph{hostname} distinto representa um domínio distinto}.
Desta forma, os domínios \texttt{www.dcc.ufmg.br} e \texttt{www.ufmg.br}
serão considerados domínios distintos mesmo que os mesmos sejam
controlados pelo menos servidor. Vários foram os motivos que nos levaram
a essa decisão. O primeiro, é que ele é o mesmo modelo adotado pelo
mecanismo \texttt{robots.txt}~\cite{robotstxt}. Depois, se para alguns
servidores compartilhados esse mecanismo aparentar ser muito agressivo,
para domínios servidos por CDNs essa decisão será por demais suave.

O conteúdo coletado será limitado apenas a conteúdo HTML. Verificações
por essa condição serão realizadas quando do envio da requisição HTTP ao
servidor e logo após a recepção do conteúdo da página. Qualquer outro
conteúdo, mesmo que seja HTML, se informado incorretamente pelo
servidor, será ignorado.

Outra decisão adotada diz respeito a forma com que lidamos com erros. Em
nossa implementação, qualquer problema que ocorra durante a requisição
de uma página, desde um erro de ``página inexistente'' até uma
solicitação de DNS que estourou o seu limite de tempo, será tratado
como erros fatal, ou seja, nenhuma nova tentativa de obter novamente
aquele recurso será realizada.

Respeitaremos tanto o padrão \texttt{robots.txt} quanto as
\emph{tags} \texttt{meta}. A solicitação do arquivo \texttt{robots.txt}
ocorrerá antes da requisição da primeira página de um domínio e
respeitará os 30 segundos de intervalo entre requisições, como comentado
na sub-seção~\ref{prob:robots}.

Finalmente, tendo em vista os pontos discutidos nas
sub-seções~\ref{prob:charmapnorm} e \ref{prob:urlnorm}, decidimos que
todo o conteúdo de páginas HTML coletado será covertido para UTF-8 antes
de que se realize o \emph{parsing} do mesmo.


\subsection{Componentes}

Os componentes da no nosso \emph{crawler} são bastante similares aos
componentes descritos na seção~\ref{sec:components}, a saber:
\begin{description}
\item[LinkExtractor:] um extrator de \emph{links} de páginas HTML;
\item[ParanoidAndroid:] a nossa implementação de um coletor de páginas
Web;
\item[DeepThought:] o nosso escalonador de requisições.
\end{description}

Nossa implementação utiliza o próprio sistema de arquivos para realizar
as funções do ``Armazém de Páginas''.

Além desses componentes clássicos, mais alguns componentes de nossa
implementação merecem menção:
\begin{description}
\item[Sauron:] nosso coletor de estatísticas;
\item[UnicodeBugger:] nosso detector e conversor de mapas de caracteres;
\item[BaseURLParser:] nosso \emph{parser} e normalizador de URLs.
\end{description}

Maiores detalhes sobre a implementação deles serão fornecidos na seção
seguinte.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPLEMENTAÇÃO %%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementação}\label{sec:implementation}

\subsection{Decisões de implementações}

Nosso \emph{crawler}, denominado de ``CrawlingBeast'' foi escrito em C++
e utiliza a biblioteca de gabaritos padrões (STL) dessa
linguagem~\cite{stroustrup97}. Para aumentar a sua escalabilidade, ele
utiliza uma abordagem \emph{multi-threading} para a coleta e
processamento de páginas.

Foram utilizadas as seguintes bibliotecas extras:
\begin{itemize}
\item libcurl, para realização de requisições HTTP;
\item zlib, para armazenamento de páginas compactadas no sistema de
arquivo;
\item gzstream, uma interface C++ para utilização da zlib;
\item libiconv, para conversão de páginas entre mapas de caracteres;
\item pthreads, para suporte a escrita aplicações com vária
\emph{threads}.
\end{itemize}

É importante observar que, excetuando-se a \emph{gzstream} e a
\emph{pthreads}, todas as outra bibliotecas já são dependências da
\emph{libcurl}.

Além disso, nosso código faz uso extensivo de uma técnica para controle
de recursos (como memória, \emph{locks} etc)  muito comum em C++,
denominada RAII (\emph{Resource Acquisition Is Initialization}).
Além disso, como comentaremos mais a frente, diversas partes de nosso
ccódigo possuem código de teste (\emph{Unit Testing}).

\subsection{Prototipação e \emph{Unit Testing}}

Antes do inicio da implementação em C++ foi implementado um protótipo em
Python do \emph{crawler}. Python é uma linguagem dinâmica (ou
interpretada), orientada a objetos, com tipagem dinâmica, tipos de dados
dinâmicas de alto nível, com suporte a exceções e com uma excelente
biblioteca de classes auxiliares~\cite{pythonlang}.

Devido às características dessa linguagem, foi possível implementar em
um curto espaço de tempo todas as principais funcionalidades do
\emph{crawler}, em especial o \emph{parser} e extrator de \emph{links},
o \emph{parser} e normalizador de URLs, e toda a arquitetura relacionada
ao escalonador de URLs.

O objetivo da prototipação era de poder experimentar e testar, de
maneira rápida e em uma ambiente mais flexível do que o oferecido pelo
C++, as nossas soluções propostas para os desafios encontrados no
desenvolvimento de um crawler.

Além disso, diversas características da linguagem encorajam o
desenvolvimento de código modular, documentado e com código de testes
integrado, tudo isso em um modelo de orientação a objetos
similar ao do C++. Devido a isso, várias das boas práticas de
programação encorajadas pelo ambiente de prototipação foram herdadas no
código em C++. Uma dessas heranças foi um conjunto substancial de \emph{unit
tests} criados para os
\emph{parsers} de HTML e de URLs, que checam um conjunto extensivo de
erros de mal-formidade para esses dois padrões. No código em C++ a
solução de \emph{unit testing} adotada foi o CxxTest~\cite{cxxtest}.

Tanto o código do protótipo quanto o código das Unit Teste geradas para
ambos os códigos encontra-se anexado a esse documento.

\subsection{Estruturas de dados}

Nossa implementação usa as seguintes estruturas de dados para gerenciar
o escalonamento de URLs~\cite{cormen-algorithms}:

\begin{itemize}
\item Uma \emph{hashtable} de domínios conhecidos. Busca e inserção
nessa estrutura possui uma complexidade de tempo médio de \(O\left(1 
\right)\).
\item Uma lista de prioridades de domínios pendentes, implementada como
uma \emph{head}. Inserção, e remoção nessa estrutura possuem
complexidades de tempo no pior caso de \(O\left(\log n\right)\). A
localização do elemento de maior prioridade leva tempo constante.
\end{itemize}

Cada domínio, por sua vez, apresenta estruturas similares:
\begin{itemize}
\item Um conjunto de \emph{paths} conhecidos dentro daquele domínio,
implementado como um \emph{hashset} e possuindo as mesmas complexidades
de uma \emph{hashtable}.
\item Uma fila FIFO de \emph{paths} pendentes nesse domínio, ou seja de
\emph{paths} que precisam ser coletados. Essa lista é implementada como
uma lista duplamente-encadead e possui tempo constante para remoção do
primeiro elemento e parar acréscimo ao fim de um novo elemento.
\end{itemize}


\subsection{Componentes}

Nesta seção discutiremos decisões de implementação, problemas e soluções
encontradas no desenvolvimento de cada um dos componentes da nossa
solução.

\subsubsection{Parser}

O parser deveria ser mais do que um simples extrator de URL de tags 'a'.

Como não era possível usar um parser HTML pronto nem depender de expressões
regulares, acaba sendo necessário então escrever um pequeno parser \emph{push}
para HTML.

Toda a infra-estrutura criada para esse parar será futuramente re-utilizada
para o indexador. Dessa forma, preferimos no extender um pouco mais na sua
criação, garantindo assim ganho de tempo na versao do indexador e, ao mesmo
tempo, garantindo que o nosso parser seja mais do que capaz de lidar com a
tarefa que lhe é solicitada no momento.

Observer os documentos HTML existentes na Web atual não totalmente válidos e/ou
bem formado se confrontados com os padroes existentes e publicados pela W3C
para html. Mais Do que isso, os documentos da web atual são uma mistura de
documentos HTML e XML e nem todos eles são documentos bem formados. Dessa forma

\begin{itemize}

\item Devido a essa mescla de XML e HTML, o parser deveria ser capaz de lidar
com a maioria das construções de XML e HTML e ser capaz de lidar com documentos
de se situam no interim desses dois padrões, mesmo em se tratando de erros.

\item Mais do que isso, o parser deve ser capaz de lidar dom

\item o parser deveria ser bem toleante a erros e tolerar documentos
mal-formados da mesma forma com que os navegadores toleram. 

Oscilar entre os padrões.... Tags de fechamento, de abertura, atributos sem
valor, com seu conteudo não delimitado por \" ou \'. Tentamos conciliar o
compromisso de pegar todo o texto que um navegador fosse capaz de interpretar
mas lidar com erros corretamente.

\end{itemize}

Dessa forma, para que o parser cumprisse sua função de extrair o máximo de
informação útil possível de páginas, de maneira similar ao navegadores. Para
isso, tentamos garantir que ele se comporta-se tão próximo dos modelos de
fererência quanto possível (Firefox, BeautifulSoup). Entretanto, isso nos levou
a algumas escolhas quanto á forma de lidar com erro que vão de encontro até
mesmo com a especificação de XML e HTML.

No fragmento abaixo, de acordo com as especificaçõe de XML, não existe nenhuma tag:
%\texttt{ a#whatever duplas="x" simples='what' html=antigo attrhtml / }

Isso deve-se pela especificação do que pode ser o nome (\emph{Name}) de uma
tag, e o caractere \& não faz parte dele. Entretanto, para esse exemplo, o
firefox reconhece a existência de uma tag 'a\&whatever', enquanto o
BeautifulSoup reconhece a existência de uma tag 'a'.

Nossa queremos seguir o firefox tanto quanto possível.

\subsubsection{Tags Especiais}

Script, Style, textarea e blocos XML CDATA não devem ter seu conteúdo
processados pelo parser.

Tags empty (que fecham sozinhas). Por isso, nosso parser difere de outros por
já sinalizar que uma tag é auto-closing. Isso permite ao programador
diferenciar se essa é uma Start Tag normal, o que torna possívevl ao usuário
avançar forçosamente o texto para o local onde a tag de fechamento está,
ignorando assim seu conteúdo e protegendo o próprio parser, ou se ela é uma tag
Empty, o que não torna necessário nenhuma intervenção do usuário.

\subsubsection{URL}

RFC 3986

http://en.wikipedia.org/wiki/URL\_normalization
http://en.wikipedia.org/wiki/Percent-encoding



% vim:fileencoding=utf-8:syn=tex:ai:tw=72:smartindent:
% vim:spell:spelllang=pt:

\subsubsection{Detecção de Mapas de Caracteres}

\subsubsection{Mapas de Caracteres, Unicode e Codificação de Texto}
BOM, UTF-8

\subsubsection{Problemas na decodificação de textos na Web}

\subsubsection{Solução encontrada}

KISS.

Tentaremos seguir a RFC 3023.
\subsection{Políticas de blah para robôs}
http://www.robotstxt.org/wc/meta-user.html

rel=``nofollow'' http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html  



\subsubsection{Limitando o conteúdo baixado}

Apenas HTML 

w3c - xhtml, seção 5.
RFC 3236
http://www.w3.org/TR/2002/NOTE-xhtml-media-types-20020801/

\subsection{Dificuldades}

\subsection{Considerações sobre Gerência de Memória de Regiões críticas}

Resource Aquisition Is Initialization.

\subsection{Problemas encontrados e Melhorias futuras}

Buracos negros - nossa abordagem de escalonamento foi baseada em domínios. Isso
permitia não somente controlar o tempo mínimo de intervalo entre acessos
consecutívos a um mesmo domínio, limitar o uso de memória (já que nao era mais
necesári armazenar o nome de domínio em cada URL pertencente a ele) e armazenar
a lista de URLs pendentes. Todavia, para a nossa implementação, sítios que
fazem uso extensivo de sub-domínios para sua organização são torna um problema.
Isso deve-se ao fato de que, custumeiramente, a obtenção e \emph{parsing} de
uma unica página de um sítio destes acarreta na ``descoberta'' de vários
sub-domínos desse sítio, o que por sua vez acarreta no acréscimo de cada um
destes na lista de domínios conhecidos e de domínios pendentes. Além desses
domínios serem costumeriramente rasos, ou seja, sem muitos links para
documentos internos, a sua adição limitará a capacidade do crawler de baixar
páginas de conteúdo, uma vez que, antes de que qualquer conteúdo possa ser
obtido desses domínios, os arquivos \texttt{robots.txt} dele terá de ser
obtido.

Libcurl e HTTPS.

Uso mais agressivo de identificadores numéricos para os arquivos. Isso será
útil não somente para a contenção dos gastos com memória mas também quando
estivermos realiando a indexação dos documentos.

Melhorias na listas de prioridades.

duas listas de prioridade: uma para dominios que não podem ser baixados
ainda (timestamp muito recente) e outra para domínios disponíveis. Nessa
última a lista de prioridade usaria o tamanho da fila pendente, o que,
segundo Baesa-Yattes, resulta em um bom crawling\ldots

 Os algoritmos e estruturas disponíveis na
STL para lidar com listas de prioridades e heaps não possuem métodos para
decrementar ou incrementar um o valode atribuído a um ítem interno à heap.
Dessa forma, torna-se inviável a construção de mecanismos que implementem
políticas de priorização de domínios ou de páginas baseados na quantidade de
links que apontam para os mesmos. Esse mecanismo, que pode ser interpretado
como uma versão consideravelemnte ingênua das idéias por trás de algorítmos
como o PageRank, poderiam contorar de maneira elegantes os buracos negros de
domínio

% Bibliografia
\nocite{stroustrup97}
\bibliographystyle {plain}
\bibliography{relatorio}


\end{document}

% vim:tw=72 fileencoding=utf-8 spelllang=pt spell syn=tex:
