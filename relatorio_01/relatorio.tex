\documentclass[10pt,twocolumn]{article}

\usepackage{graphicx,url}
\usepackage{subfigure}

\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{subfigure}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Header  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Trabalho prático 1}
\author{Tiago Alves Macambira \\ \texttt{tmacam@dcc.ufmgbr}}
\date{4 de maio de 2007}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Body  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Introdução}

O primeiro trabalho prático da disciplina de Recuperação de Informação
consiste na elaboração de um \emph{crawler} industrial para páginas Web.
Esse \emph{crawler} deveria atender vários requisitos, como disposto no
enunciado do trabalho~\cite{tp1}, dentre os quais podemos citar:
\begin{itemize}
\item O sistema deveria ser capaz de coletar 1~milhão de URLs por dia
utilizando um servidor padrão,
\item O coletor deve respeitar as ``regras de etiqueta para
\emph{crawling} de páginas \emph{Web}''.
\item A velocidade de coleta de deve se manter constante
\item O coletor deve ser escrito em C/C++. Bibliotecas extras somente
poderão ser utilizadas mediante autorização dos professores.
\end{itemize}

\subsection{Terminologia}

No restante desse relatório serão utilizados alguns termos de uso
não-comum. Afim de evitar possíveis problemas de interpretação,
disporemos seus significados aqui.

\begin{description}
\item[URL pendente:] Uma URL que foi descoberta mas não foi requisitada aos seu
servidor.
\item[Domínio:] Um domínio nada mais é do que um nome de domínio DNS
(DNS \emph{domainname}) atrelado a um
endereço IP. Essa mesma terminologia é costumeiramente utilizada quando
tratamos de DNS~\cite{rfc1034}.
\item[Parsing:] Leitura de uma página em que é realizada a interpretação do
conteúdo nela presente, geralmente consistindo de marcação de conteúdo HTML e,
no caso de arquivos \texttt{robots.txt}, de conteúdo com sintaxe de arquivos
\texttt{robots.txt}.
\item[Sítio:] Uma localidade na Internet, geralmente representada um um domínio
e um conjunto de sub-domínios. Apesar de ter o mesmo significado do termo
inglês \emph{site}, preferimos utilizar o nome em português. É importante
salientar que um sítio pode ser composto por mais de um domínio.
\item[Tag] Elemento de marcação utilizado em documentos HTML e XML.
\end{description}

\section{\emph{Crawling} de páginas web}

\emph{Web crawlers}, também conhecidos como \emph{Web spinders} ou
\emph{Web robots} são programas ou sistemas automatizados que percorrem
a \emph{Web} de maneira automatizada. Esses programas estão geralmente
associados a sistemas de máquinas de busca e, nesse contexto, têm como
objetivo coletar páginas da WWW para serem posteriormente indexadas.

Durante o processo de coleta de páginas, os \emph{crawlers} vão
descobrindo novas URLs que serão enviadas a um gerente e futuramente
escalonadas para obtenção.

\subsection{Componentes de um crawler}\label{sec:components}

De forma resumida, um \emph{crawler} para a \emph{Web} possui os
seguintes componentes:

\begin{description}

\item[Coletores de páginas:] São os elementos que efetivamente realizam
a coleta de página. Sistemas de \emph{crawling} para a \emph{Web}
geralmente empregam vários coletores em paralelo para aumentar a sua
capacidade de coleta de páginas. Esses coletores podem ser responsáveis
apenas pela realização das requisições HTTP como podem ser também
responsáveis pela realização de algum tipo de pre-processamento das
páginas.

\item[Extrator de links:] É o elemento que realiza o \emph{parsing} dos
documentos coletados, extraindo \emph{links} destes. Esse componente
também é responsável por normalizar as URLs encontradas no documento bem
como por resolver URLs relativas presentes no mesmo. Apesar de ter um
papel definido, esse elemento pode se encontrar contido ou associado a
elementos coletores de páginas.

\item[Escalonador de Requisições:] As URLs extraídas pelo elemento
anterior são reportadas a esse componente. É papel do
escalonador gerenciar e distribuir URLs a serem vasculhadas pelos
coletores. Esse processo de seleção deve garantir que os sítios
visitados não sejam soterrados por requisições, que essas requisições
respeitam as politicas indicativas para coleta dispostas nos sítio e que
a coleção de páginas baixadas tenha uma amostra relevante das páginas na
Internet.

\item[Armazém de Páginas] As páginas coletadas podem não sofrer 
de imediato toda a
análise necessária para os propósitos de uma máquina de busca. Assim
sendo, será necessário guardar essas páginas para posterior
processamento. Esse componente de um sistema de \emph{web crawling} é o
que fica responsável por essa tarefa. Observe que a existência desse
componente não é estritamente necessário para um \emph{crawler} mas é
fundamental para uma máquina de busca, uma vez que a indexação pode
ocorrer desassociada da extração de links.
\end{description}

\subsection{Problemas e desafios}

Apesar da aparente simplicidade, a realização de coleta de páginas na
WWW apresenta alguns desafios particulares que, se não forem tratados,
podem comprometer a capacidade do coletor ou até mesmo inviabilizar o
resultado de sua coleta. 

Nessa seção comentaremos sobre alguns desses problemas. Na
seção~\ref{sec:implementation} comentaremos como lidamos com esses
problemas na nossa implementação.

\subsubsection{Resolução de nomes de domínios}\label{prob:dns}
Uma vez de posse de uma lista de URLs a serem coletadas, cada coletor
deve então resolver cada nome do domínio existente nas URLs da sua lista
para um endereço IP, para que ele possa então entrar em contato com o
servidor da página desejada e solicitá-la. Dependendo da velocidade de
coleta do \emph{crawler} e da dispersão dos nomes de domínios em suas
listas de URLs, cada coletor passará um tempo considerável no processo de
resolução de nome.

A utilização de \emph{caches} pode aliviar esse problema, mas é
importante ter em mente que o sistema de resolução de nomes na Internet,
o DNS, permite que nomes de domínio tenham uma valida tão ínfima quanto
o tempo de duração da própria requisição, o que vai de encontro com a
utilização de \emph{caches}.

\subsubsection{Normalização de mapas de
caracteres}\label{prob:charmapnorm}

As diferentes línguas utilizadas pelo homem usam distintos conjuntos de
símbolos para sua representação e, para vários desses conjunto de
símbolos, mais de uma forma de representação binária existe. Esse fato
torna complicado a troca e a interpretação de texto em línguas diferentes
e mesmo para documentos escritos na mesma língua. Para a língua inglesa,
por exemplo, que utiliza um conjunto de caracteres bem limitado, existem
dois conjuntos de mapas de caracteres históricos diferentes e
incompatíveis, ASCII e EBCDIC. Se contarmos os mapas de caracteres e
formas de codificação diferentes existentes para as várias línguas
humanas, esse problema torna-se bem mais complexo.
Mesmo a existência de conjuntos ``universais'' como UTF-8, UTF-16 e
UTF-32 não resolvem esse problema, pois nem todos os documentos na
Internet se encontram codificados nesses mapas de caracteres.

É salutar observar que a capacidade de interpretar corretamente
documentos escritos em mapas de caracteres distintos e de recodificá-los
para um mapa de caractere comum  repercute não somente na capacidade de
realizar \emph{parsing} de documentos, mas também na capacidade de
indexação e extração de \emph{links} do mesmo, uma vez a forma de
representar caracteres irá afetar diretamente a capacidade de
normalizar o vocabulário e de normalizar URLs.

\subsubsection{Descoberta de mapa de
caracteres}\label{prob:charmapdetection}

Associado ao problema de normalizar os documentos em um único mapa de
caracteres está o problema de, dado um documento, descobrir em qual mapa
de caracteres o mesmo se encontra para que se possa então realizar a
tradução. Informações sobre o mapa de caractere usado por uma página web
podem ser fornecidas de diversas formas:
\begin{itemize}
\item por cabeçalhos da requisição HTTP~\cite{rfc2616},
 o protocolo de aplicação utilizado na \emph{Web},
\item pelo prólogo de um documento XML~\cite{bray2006xml},
\item através de uma \emph{tag} \texttt{Meta} em um documento
(x)HTML~\cite{html4tr}.
\end{itemize}

Entretanto, os padrões acima apresentam sobreposições, colocando o
desenvolvedor em situações de impasse. Por exemplo, quando mais de um
padrão fornece informações conflitantes, qual utilizar?  Quando nenhum
mecanismo acima informa nada sobre o mapa de caracteres usado, qual
padrão ou qual regra usar por omissão?

%http://feedparser.org/docs/character-encoding.html
%    COmo é feito no XML
%        Mas ele tem UTF8 como default
%        http://www.w3.org/TR/REC-xml/\#sec-guessing-no-ext-info
%    HTTP defaults to ISO 8859-1 (Latin 1)
%    O que a RFC 3023 diz? HTTP  Headers -> XML Dec. -> UTF-8
%
% Windows-1252 (http://en.wikipedia.org/wiki/ISO\_8859-1)

Uma alternativa plausível é realizar o mesmo processo utilizado por
navegadores \emph{Web}, que é uma extensão do modelo proposto pela RFC
3023~\cite{rfc3023}. Esse processo consiste em seguir os passos abaixo,
parando no primeiro que tenha suas restrições atendidas:
\begin{enumerate}
\item Usar as informações dos cabeçalhos HTTP, sempre que possível e
sempre que estes estejam disponíveis.
\item  Se os primeiros 2--4 bytes forem macadores de ordem de bytes
(BOM, de \emph{Byte Order Mark}) XML ou UTF, a codificação
correspondente será utilizada.
\item Se o documento iniciar com um prólogo XML legível e que informe a
codificação do documento, esse será utilizado.
\item Se o documento conter a tag HTML \texttt{meta}, como em \texttt{<meta
http-equiv="Content-Type" ...> }, o mapa de caracteres declarado nessa
tag será usado.
\item Finalmente, como última opção, existe a abordagem de detecção de
mapas de caracteres por heurísticas compostas~\cite{mozillaiuc}.
\end{enumerate}

\subsubsection{\emph{Parsing} de páginas}\label{prob:html}

Apesar de ser um problema de relevância maior no processo de indexação
de documentos, o \emph{parsing} de documentos HTML também é um problema
a ser enfrentado por um extrator de \emph{links}.

Os documentos HTML existentes na \emph{Web} atual geralmente não são
considerados válidos
e/ou bem formados se confrontados com os padrões existentes e publicados
pela W3C para HTML e XHTML~\cite{html4tr, bray2006xml}, sendo comumente
uma mistura dos dois padrões. Uma busca por URLs de maneira simples como
através do uso de expressões regulares, se eficiente, mostra-se ingênua
quando deparada com documentos HTMLs mais complexos. Não somente uma
abordagem dessas pode ignorar seções como comentários HTML/XML, mas
também ignorar \emph{tags} espciais do HTML tais como a \texttt{SCRIPT},
\texttt{TEXTAREA} e \texttt{STYLE}, que, se presentes, podem conter
conteúdo que não é HTML e que não deve ser recuperado por um extrator de
URLS.

\subsubsection{Normalização de URLs}\label{prob:urlnorm}

Outro aspecto importante de um \emph{crawler} é a sua capacidade de
normalizar URLs. A normalização serve a vários propósitos mas, sobretudo, para
evitar a duplicação de esforços na obtenção de páginas que são
essencialmente a mesma. Existem várias técnicas que podem ser utilizadas
para o processo de normalização de URLs, cada uma delas se aplicando a
cada um dos 5 componentes distindos de uma URL: esquema, autoridade,
\emph{path}, \emph{query} e fragmento. Várias dessas técnicas são
tratadas na RFC 3986~\cite{rfc3986}.

%\subsubsection{Obtenção de páginas com conteúdo dinâmico}
%Crawling de páginas dinâmicas. O que constituem páginas dinâmicas.
%Por que baixá-las ou evitá-las.

\subsubsection{Boas-práticas para \emph{crawling} de
páginas}\label{prob:robots}

A atividade de coleta de um sítio por um \emph{crawler} não pode ser por
demais onerosa ao primeiro. Caso contrário, a coleta poderá inviabilizar
o funcionamento normal de um sítio e haverá motivos para que os
administradores desse sítio bloqueiem acessos futuros dos coletores a
ele. Em outra situações, os administradores de um sítio podem determinar
que eles não desejam ter parte ou todo o seu sítio indexado ou visitado
pelo \emph{crawler}.

Para solucionar esse problema de maneira ``cordial'', foram
desenvolvidos dois mecanismos que indicam aos coletores de páginas quais
restrições os administradores de um domínio aplicam às suas páginas. O
primeiro e mais antigo consiste em colocar um arquivo, entitulado
\texttt{robots.txt} na raiz dos dominínios indicando quais páginas podem
ou não ser coletadas~\cite{robotstxt}. O segundo consistem em colocar
uma \emph{tag} \texttt{meta} especial no cabeçalho do arquivo HTML que
informa se aquela página pode ou não ser indexada e se pode ou não ser
seguida, ou seja, se os links contidos naquela página podem ser
acrescentados à lista de \emph{links} de um
\emph{crawler}~\cite{robotsmeta}.

Além disso, um dos requisitos desse trabalho era que nenhuma requisição
a um dado servidor fosse realizada em um intervalo inferior a 30 segundos.


%rel=``nofollow'' http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html  

%\subsubsection{Escalonamento de URLs e gerência de URLs visitadas}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROJETO %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{O projeto do \emph{CrawlerBeast}}

Realizamos o projeto de nosso \emph{crawler} tendo em mente os desafios
mencionados na seção anterior. Algumas decisões de projeto tiveram que
ser tomadas tanto para contornar tais problemas como para ser capaz de
completar o trabalho no prazo estipulado.

\subsection{Premissas e decisões iniciais de projetos}

Uma das primeira decisões de projeto foi a forma como lidaríamos com o
requisito de intervalo mínimo de 30 segundos. Optamos por considerar que
cada \textbf{\emph{hostname} distinto representa um domínio distinto}.
Desta forma, os domínios \texttt{www.dcc.ufmg.br} e \texttt{www.ufmg.br}
serão considerados domínios distintos mesmo que os mesmos sejam
controlados pelo menos servidor. Vários foram os motivos que nos levaram
a essa decisão. O primeiro, é que ele é o mesmo modelo adotado pelo
mecanismo \texttt{robots.txt}~\cite{robotstxt}. Depois, se para alguns
servidores compartilhados esse mecanismo aparentar ser muito agressivo,
para domínios servidos por CDNs essa decisão será por demais suave.

O \textbf{conteúdo coletado será limitado apenas a conteúdo HTML}.
Verificações por essa condição serão realizadas quando do envio da
requisição HTTP ao
servidor, pelo envio de um cabeçalho \texttt{Accept}, e logo após a
recepção do conteúdo da página, pela verificação do conteúdo do
cabeçalho \texttt{Content-Type}. A segunda
verificação é necessária pois nem todos os servidores respeitam a
condição imposta pelo cabeçalho \texttt{Accept} enviado na requisição da
página. Qualquer outro
conteúdo ou mesmo conteúdo HTML, caso este seja informado incorretamente pelo
servidor, será ignorado. Em nossa implementação são reconhecidos como
conteúdo HTML páginas servidos com os seguintes tipos
MIME~\cite{xhtml1, html4tr, xhtmlmediatypes, rfc3236}:
\texttt{text/html}, \texttt{application/xhtml+xml} e conteúdo do tipo
\texttt{application/\emph{*}-xml}.

Outra decisão adotada diz respeito a forma com que lidamos com erros. Em
nossa implementação, \textbf{qualquer problema que ocorra durante a requisição
de uma página}, desde um erro de ``página inexistente'' até uma
solicitação de DNS que estourou o seu limite de tempo, \textbf{será tratado
como erros fatal}, ou seja, nenhuma nova tentativa de obter novamente
aquele recurso será realizada.

Respeitaremos tanto o padrão \texttt{robots.txt} quanto as
\emph{tags} \texttt{meta}, quando este estiverem disponíveis nos sítios
e nas páginas visitadas. A solicitação do arquivo \texttt{robots.txt}
ocorrerá antes da requisição da primeira página de um domínio e
respeitará os 30 segundos de intervalo entre requisições, como comentado
na sub-seção~\ref{prob:robots}.

\textbf{Páginas dinâmicas serão visitadas com o conteúdo do componente
\emph{query} das sua URLs removidos.} Esta pode não ser a melhor maneira
de lidar com \emph{crawling} de conteúdo dinâmico na \emph{Web} mas é
apresenta-se como uma boa solução de compromisso.

Finalmente, tendo em vista os pontos discutidos nas
sub-seções~\ref{prob:charmapnorm} e \ref{prob:urlnorm}, decidimos que
\textbf{todo o conteúdo de páginas HTML coletado será convertido para
UTF-8} antes
de que se realize o \emph{parsing} do mesmo. Além disso, o conteúdo será
salvo compactado, para economizar acessos ao disco.


\subsection{Componentes}

Os componentes da no nosso \emph{crawler} são bastante similares aos
componentes descritos na seção~\ref{sec:components}, a saber:
\begin{description}
\item[LinkExtractor:] um extrator de \emph{links} de páginas HTML;
\item[ParanoidAndroid:] a nossa implementação de um coletor de páginas
Web;
\item[DeepThought:] o nosso escalonador de requisições.
\end{description}

Nossa implementação utiliza o próprio sistema de arquivos para realizar
as funções do ``Armazém de Páginas''.

Além desses componentes clássicos, mais alguns componentes de nossa
implementação merecem menção:
\begin{description}
\item[Sauron:] nosso coletor de estatísticas;
\item[UnicodeBugger:] nosso detector e conversor de mapas de caracteres;
\item[BaseURLParser:] nosso \emph{parser} e normalizador de URLs.
\end{description}

Maiores detalhes sobre a implementação deles serão fornecidos na seção
seguinte.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPLEMENTAÇÃO %%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementação}\label{sec:implementation}

\subsection{Decisões de implementações}

Nosso \emph{crawler}, denominado de ``CrawlingBeast'' foi escrito em C++
e utiliza a biblioteca de gabaritos padrões (STL) dessa
linguagem~\cite{stroustrup97}. Para aumentar a sua escalabilidade, ele
utiliza uma abordagem \emph{multi-threading} para a coleta e
processamento de páginas.

Foram utilizadas as seguintes bibliotecas extras:
\begin{itemize}
\item libcurl, para realização de requisições HTTP;
\item zlib, para armazenamento de páginas compactadas no sistema de
arquivo;
\item gzstream, uma interface C++ para utilização da zlib;
\item libiconv, para conversão de páginas entre mapas de caracteres;
\item pthreads, para suporte a escrita aplicações com vária
\emph{threads}.
\end{itemize}

É importante observar que, excetuando-se a \emph{gzstream} e a
\emph{pthreads}, todas as outra bibliotecas já são dependências da
\emph{libcurl}.

Além disso, nosso código faz uso extensivo de uma técnica para controle
de recursos (como memória, \emph{locks} etc)  muito comum em C++,
denominada RAII (\emph{Resource Acquisition Is Initialization}).
Além disso, como comentaremos mais a frente, diversas partes de nosso
ccódigo possuem código de teste (\emph{Unit Testing}).

\subsection{Prototipação e \emph{Unit Testing}}

Antes do inicio da implementação em C++ foi implementado um protótipo em
Python do \emph{crawler}. Python é uma linguagem dinâmica (ou
interpretada), orientada a objetos, com tipagem dinâmica, tipos de dados
dinâmicas de alto nível, com suporte a exceções e com uma excelente
biblioteca de classes auxiliares~\cite{pythonlang}.

Devido às características dessa linguagem, foi possível implementar em
um curto espaço de tempo todas as principais funcionalidades do
\emph{crawler}, em especial o \emph{parser} e extrator de \emph{links},
o \emph{parser} e normalizador de URLs, e toda a arquitetura relacionada
ao escalonador de URLs.

O objetivo da prototipação era de poder experimentar e testar, de
maneira rápida e em uma ambiente mais flexível do que o oferecido pelo
C++, as nossas soluções propostas para os desafios encontrados no
desenvolvimento de um crawler.

Além disso, diversas características da linguagem encorajam o
desenvolvimento de código modular, documentado e com código de testes
integrado, tudo isso em um modelo de orientação a objetos
similar ao do C++. Devido a isso, várias das boas práticas de
programação encorajadas pelo ambiente de prototipação foram herdadas no
código em C++. Uma dessas heranças foi um conjunto substancial de \emph{unit
tests}, que em C++ totalizam 105 testes distintos,
 criados sobretudo para os
\emph{parsers} de HTML e de URLs, que checam um conjunto extensivo de
erros de mal-formidade para esses dois padrões. No código em C++ a
solução de \emph{unit testing} adotada foi o CxxTest~\cite{cxxtest}.

Tanto o código do protótipo quanto o código dos \emph{unit tests} gerados para
ambos os códigos encontram-se anexados a esse documento.

\subsection{Políticas de escalonamento}

Durante a elaboração e testes da nossa implementação, realizamos testes
com duas política de escalonamento de URLs distintas:
\begin{description}
\item[Breadth-First (BF):] As páginas e os domínios são vizitados na ordem que
são encontrados no processo de \emph{crawling};
\item[Largest Site First (LSF):] Os domínios são visitados de modo a priorizar
aqueles que possuem a maior lista de URLs pendentes até aquele momento.
\end{description}

Em seu artigo à WWW de 2005, Beaze-Yates discute as vantagens e
desvantages desses e de outros mecanismos de
escalonamento~\cite{baezayates2005crawling}.



\subsection{Estruturas de dados}\label{sec:datastructures}

Para a política BF, nossa implementação usa as seguintes estruturas de
dados para gerenciar o escalonamento de URLs~\cite{cormen-algorithms}:

\begin{itemize}
\item Uma \emph{hashtable} de domínios conhecidos. Busca e inserção
nessa estrutura possui uma complexidade de tempo médio de \(O\left(1 
\right)\).
\item Uma fila de prioridades de domínios pendentes, implementada como
uma \emph{heap} ordenada pelo tempo da última visita ao domínio.
Inserção, e remoção nessa estrutura possuem
complexidades de tempo no pior caso de \(O\left(\log n\right)\). A
localização do elemento de maior prioridade leva tempo constante.
\end{itemize}

Para a política LSF, nossa implementação usa as seguintes estruturas de
dados para gerenciar o escalonamento de URLs:

\begin{itemize}

\item Uma \emph{hashtable} de domínios conhecidos. Busca e inserção
nessa estrutura possui uma complexidade de tempo médio de \(O\left(1 
\right)\).

\item Uma fila de domínios inativos. Essa fila contêm domínios que
anda não podem ser coletados pois o intervalo mínimo de tempo entre
visitas consecutivas para eles ainda não foi atingido. Essa fila foi
implementada como uma fila com terminação dupla (DEQue, ou \emph{Double
Ended Queue}), e a inserção e remoção de nós nessa lista leva tempo
constante. Por construção, ela possui uma lista de domínios ordenados
pelo tempo da última visita de maneira crescente.

\item Uma fila de prioridades de domínios ativos. Essa estrutura contém
uma lista de todos os domínios que podem ser visitados naquele instante
e é implementada como uma \emph{heap} ordenada pelo tamanho da fila de
URLs pendentes de cada domínio. Inserção e remoção nessa estrutura
possuem complexidades de tempo no pior caso de \(O\left(\log n\right)\).
A localização do elemento de maior prioridade leva tempo constante.

\end{itemize}


Cada domínio, por sua vez, apresenta a mesma estrutura em ambas as
políticas de escalonamento:
\begin{itemize}
\item Um conjunto de \emph{paths} conhecidos dentro daquele domínio,
implementado como um \emph{hashset} e possuindo as mesmas complexidades
de uma \emph{hashtable}.
\item Uma fila FIFO de \emph{paths} pendentes nesse domínio, ou seja de
\emph{paths} que precisam ser coletados. Essa lista é implementada como
uma lista duplamente encadeada e possui tempo constante para remoção do
primeiro elemento e para acréscimo ao seu final de um novo elemento.
\end{itemize}


\subsection{Componentes}

Nesta seção discutiremos decisões de implementação, problemas e soluções
encontradas no desenvolvimento de cada um dos componentes da nossa
solução.

\subsubsection{\emph{Parsers}}

Tanto a extração de \emph{links} como a interpretação e normalização de
URLs apresentam algumas características em comum. Para aproveitar esse
fato e evitar duplicação código, foi criada uma pequena infra-estrutura para
a construção de \emph{parsers} recursivos-descendentes
(\texttt{BaseParser}). Essa implementação busca evitar cópias
desnecessárias de dados ao fazer uso extensivo de uma estrutura própria
(\texttt{filebuf}) em lugar de strings C++ convencionais.

\subsubsection{\emph{Parsing} de documentos HTML e extração de
\emph{links}}

O \texttt{LinksExtractor} é nossa implementação de um extrator de
\emph{links} em um documento HTML. Ele é construído sobre um
\emph{parser} genérico para documentos XML e HTML, o
\texttt{SloppyHTMLParser}. Ele recupera \emph{links} em \emph{tags}
\texttt{LINK}, \texttt{IFRAME}, \texttt{FRAME}, \texttt{AREA}, além da
tradicional \emph{tag} \texttt{A}.

O \texttt{SloppyHTMLParser}, por sua vez foi construído utilizando
infra-estrutura provida pelo \texttt{BaseParser} e é na verdade um
\emph{parser} completo e versátil para documentos HTML e XML. Sua
elaboração foi feita tendo em vista as especificações dos dois padrões
de documentos acima e o comportamento que navegadores \emph{Web}
tradicionais apresentam quando defrontados com documentos mal-formados.
A interface provida para seus utilizadores é similar à de \emph{parsers}
SAX XML tradicionais~\cite{saxxml, bray2006xml, html4tr}. Ao contrário
destes últimos, ele e é capaz de lidar com uma ampla variedade de
construções XML e HTML mesmo em documentos
mal-formados. Além disso, ele reconhece \emph{tags} HTML que devem ter
seu conteúdo ignorado por \emph{parsers} desse tipo de documento, como
comentado na seção~\ref{prob:html}.

Toda a infra-estrutura criada para o \texttt{SloppyHTMLParser} será
futuramente re-utilizada para o indexador. Dessa forma, preferimos nos
extender um pouco mais na sua criação, garantindo assim ganho de tempo
na versão do indexador e, ao mesmo tempo, garantindo que o nosso
\emph{parser} e as ferramentas que o usam, como o
\texttt{LinkExtractor},  sejam mais do que capazes de lidar com as
tarefas que lhe são solicitadas no momento.

\subsubsection{Interpretação e Normalização de URLs}

A interpretação e normalização de URLs fica a cargo do
\texttt{BaseURLParser}. Sua criação foi fortemente influenciada pela RFC
3986, tanto na forma de interpretar os vários componentes de uma URI mas
também na forma de realizar a normalização de cada um dos componentes de
uma URL~\cite{rfc3986}:

\begin{description}
\item[Schema:] Nosso \emph{parser} apenas lida com URLs relativas ou com URLs
absolutas com esquema HTTP. Quando presente, esse componente é
normalizado colocando-se todas as suas letras em minúsculo.

\item[Authority:] Como não interessa à nossa aplicação, o sub-componente de
informações de autenticação de usuário é descartado. Informações sobre
o \emph{hostname} e número do porto da URL, caso presentes, são
validados e normalizados. A normalização para o sub-componente
\emph{hostname} é feita colocando-se todos as suas letras em minúsculo e
removendo o último ponto no seu final, caso este exista. No caso do
porto, caso o \emph{schema} dessa URL seja HTTP e o porto tenha valor
80, esse valor é suprimido da URL final.

\item[Path:] O conteúdo desse componente é normalizado através de:
    \begin{itemize}
    \item  remoção de segmentos de ponto, seções 5.2.4 e 6.2.2.3 da RFC 3986;
    \item  normalização de codificação de porcento (\emph{Percent Encoding}), 
           seções 6.2.2.2 e 2.1;
    \item  normalização da barra final (\emph{trailing slash}), seção
           6.2.3;
    \end{itemize}

\item[Query e Fragment:] O conteúdo desses componentes não nos interessa
nesse trabalho e portanto é descartado durante o processo de
\emph{parsing}.

\end{description}

Dessa forma, durante o processo de interpretação da URL já conseguimos
normalizá-la, transformando URLs como
\begin{tabular}{c}
\texttt{\small HTtP://WWW.exemple.com.:80/ /a/c/./../../g/não/}
\end{tabular}
em
\begin{tabular}{c}
\texttt{\small http://www.exemple.com/\%20/g/n\%E3o/}
\end{tabular}

Além disso, o processo de resolução de URLs relativas implementado no
\texttt{BaseURLParser} segue fielmente o procedimento especificado na
seção 5.2.3 da RFC 3986.

\subsubsection{Detecção e Normalização de Mapas de Caracteres}

O componente \texttt{UnicodeBugger} é o responsável em nossa
implementação pela detecção e normalização de mapas de caracteres.  Todo
o conteúdo coletado, antes de ser processado, é convertido para UTF-8
por instâncias desse componente, resolvendo os problemas comentados na
seção~\ref{prob:charmapnorm}.

O processo de detecção de mapas de caracteres segue os passos descritos
anteriormente na seção na seção~\ref{prob:charmapdetection},
excetuando-se pelo último deles, o de heurísticas compostas, por ser
muito dispendiosa~\cite{mozillaiuc}.

\subsubsection{O coletor de páginas}

Na nossa implementação, cada elemento coletor de páginas nada mais é do
que uma \emph{thread} executando uma instância do componente
\texttt{ParanoidAndroid}. Esse componente é o responsável por orquestrar
o uso dos outros componentes, requisitar páginas, extrair URLs e
solicitar a resolução de URLs relativas em URLs absolutas e enviar as
novas URLs descobertas ao gerente e escalonador de nossa implementação,
o \texttt{DeepThought}.



\subsection{Dificuldades}

Nessa sub-seção discutiremos alguns dos problemas enfrentados durante a
elaboração e teste da nossa implementação.

\subsubsection{Considerações sobre Gerência de Memória de Regiões críticas}

Uma das maiores preocupações existentes durante a fase de implementação
de nosso \emph{crawler} era garantir (ou evitar) que não haveria
vazamentos de memória. Além de ser uma preocupação constante no
desenvolvimento de qualquer projeto robusto, evitar esse tipo de
problema é especialmente importante em um programa que roda por um longo
período de tempo e que re-executa o mesmo trecho de código várias vezes
por segundo.

Projetos que utilizam vários processo diferentes podem
utilizar reciclagem de processos como forma de evitar ou contornar
possíveis vazamentos. Todavia, esse não é o caso em  um sistema
\emph{multi-threaded}, uma vez que vazamentos em qualquer uma das
\emph{threads} repercute no sistema como um todo e não é possível
reciclar processos sem que todo o sistema tenha quer ser re-iniciado.

Uma das formas adotadas para evitar a todo custo esse problema foi a
adoção, em várias classes, da técnica RAII (\emph{Resource Aquisition
Is Initialization}), muito comum em C++. Essa técnica consistem em
delegar ao próprio C++, através de seus mecanismos de construção e
destruição automática de objetos na pilha, a responsabilidade de liberar
recursos -- sejam eles memória, \emph{locks} ou descritores de arquivo.

\subsubsection{libCurl}

Outro grande problema enfrentado durante a implementação foi a
utilização da libCurl. Apesar da sua API simples, essa biblioteca
mostrou-se mais problemática do que esperado por vários motivos.

Primeiro, seu uso tornava praticamente obrigatório a elaboração do
projeto como um sistema \emph{multi-threaded}, inviabilizando a construção do
mesmo como um sistema baseado em eventos e I/O assíncrono.

Segundo, apesar de possuir diretivas para assegurar que seu
funcionamento seria seguro e correto mesmo quando operando em um sistema
\emph{multi-threaded} (\emph{thread-safe}), em diversas ocasiões foi
possível observar que a biblioteca falhava em prover tais garantias.

Finalmente, algumas supostas funcionalidades criadas para ajudar o
programador que usasse essa biblioteca acabavam por criar problemas
maiores do que aqueles que ela buscava resolver. Um exemplo disso é a
capacidade da biblioteca de seguir requisições de redirecionamento do protocolo
HTTP. Ao se usar essa diretiva corria-se o risco de que a biblioteca
automaticamente tentasse estabelecer uma conexão segura (HTTPS) com um
determinado servidor. O código para estabelecimento de conexões seguras
não é \emph{thread-safe} e, nessas circunstâncias, era comum que o
sistema todo travasse devido a algum efeito maléfico de concorrência no
código tratador de HTTPS.

Por esse e por outros motivos foi necessário recompilar essa biblioteca
limitando ao máximo a capacidade dela de lidar com qualquer outro
protocolo que não HTTP.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Testes e Resultados}

\subsection{Ambiente experimental}

Todos os testes de nossa implementação foram realizados em um micro do
laboratório de graduação do DCC/UFMG, rodando como S.O. Linux Gentoo
1.12.9, kernel 2.6.19-gentoo-r5, dotado de uma CPU Pentium 4 de
3.00~MHz, 1~GB de memória RAM e 1~GB de \emph{swap}.

Todos os testes foram executados dentro da rede do DCC e utilizando os
seguintes endereços como semenstes:
\begin{itemize}
\item http://www.uol.com.br
\item http://www.ufmg.br
\item http://www.usp.br
\item http://www.unicamp.br
\item http://www.terra.com.br
\end{itemize}

\subsection{Resultados}

\begin{table}[htbp]
\centering
\begin{tabular}{|c|r|r|} \hline
Política	&  Breadth-First	& Largest Site First	\\
\hline\hline
Tempo de Coleta &   24h24m01s		&	11h50m39s	\\\hline
URLs Descobertas& & \\
	total	& 	10.806.787,00	&	6.345.287,00	\\
média por seg.	&		122,19  &	      148,81	\\\hline
URLs Visitadas	& & \\
	total	& 	  1.151.652,00	&	1.935.617,00	\\
média por seg.	&		 13,03  &	       45,40	\\\hline
URLs Coletadas	& & \\
	total	& 	    827.230,00	&	1.572.822,00	\\
média por seg.	&		  9,35  & 	       36,89	\\\hline
\hline
\end{tabular}
\caption{Dados sobre as coletas}
\label{tab:sumario}
\end{table}


A Tabela~\ref{tab:sumario} apresenta alguns valores referentes às
coletas executadas tanto com a política BF como a política LSF.
Nessa tabela, URL descoberta correspode àquelas que tiveram sua URL
mencionada em alguma página coletada. URL visitada corresponde àquelas
URLs que foram acessadas pelos coletores mas que não necessariamente
corresponderam a uma coleta bem sucedida. URLs coletadas correspondem
àquelas para as quais a sua coleta foi bem sucedida.

Uma das primeiras coisas que chama a atenção na Tabala~\ref{tab:sumario}
é que, a despeito da implementação com a política LSF ter ficado metade
do tempo no ar que a implementaçao BF, a primeira visitou e coletou
quase o dobro de páginas do que a implementação BF. Vê-se então
uma primeira vantagem da política de escalonamento LSF: ao priorizar
sítios com fila maior acaba-se priorizando sítios que, por
ter mais conteúdo, acabam tendo maior infra-estrutura
pra servir esse conteúdo. Além disso, visitar prioritariamente os mesmos
sítios (outro efeito da política LSF) também acaba aliviando a carga no DNS.

Na Figura~\ref{fig:desempenho}, apresentada em escala log-normal,
pode-se comparar ver o desempenho da
coleta no decorrer do tempo para as duas políticas de escalonamento.
Nela podemos ver mais duas vantagens da política LSF:
\begin{itemize}
\item Primeiro, a coleta com essa política consegue manter-se a uma
velocidade mais constante e, na maioria das vezes, superior ao obtido
com a política BF.
\item Ao contrário da política BF, que é sucetível a ficar presa em
sítio que utilizam vários sub-domínios para sua organização, a política
LFS mostra-se praticamente imune aos problemas acarretados por esse tipo
de prática. Comentaremos mais sobre isso na seção~\ref{dif:armadilhas}.
\end{itemize}

\begin{figure*}
  \centering
  \mbox{
    \subfigure[Breadth-Firest]{
          \label{fig:bf}
          \includegraphics[width=0.45\textwidth]{20070504-min}}
 }
 \mbox{
    \subfigure[Largest Site First]{
          \label{fig:lsf}
          \includegraphics[width=0.45\textwidth]{20070506-min}}
  }
  \caption{Desempenho do coletor com diferentes políticas de
escalonamento}
  \label{fig:desempenho}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusões}

Nesse relatório descrevemos a implementação do \emph{crawler}
especificado no TP1 da disciplina~\cite{tp1}, bem como os problemas
enfrentados e as decisões de projeto tomadas.

Pudemos observar como a política de escalonamento possui um papel
fundamental no desempenho do coletor. Em nossos experimentos, o
uso da política ``\emph{largest site first}'' mostrou-se capaz de, em
50\% do tempo necessário, baixar 150\% do conteúdo desejado para um
único dia de coleta. Além disso, mostro-se quase duas vezes mais eficiente do que a
política ``\emph{breadth-first}''. Esses resultados, embora bastante
interessantes, estão longe de ser novidades, sendo bastante estudados na
literatura~\cite{baezayates2005crawling}.


Finalmente, foi também possível observar como que, a despeito de ser
aparentemente um problema simples, a elaboração de um \emph{crawler}
industrial é complicada.

\subsection{Problemas encontrados e Melhorias
futuras}\label{sec:problemas}

Durante a implementação do \emph{crawler}, diversos problema foram
observados mas, devido a problema relativos ao tempo de entrega do
trabalho, tiveram que ser deixados de lado. Todavia, esses problemas
merecem menção, uma vez que constituem limitações da implementação e
que, em TP's futuros podem se revelar como problemas cuja solução será
necessária.

\subsubsection{Armadilhas para \emph{crawlers}}\label{dif:armadilhas}

Um dos problemas de se lidar com a coleta de conteúdo dinâmico é que
sítios com esse tipo de página pode conter o que se chama e ``buracos
negros'', ou seja, é possível que o coletor possa se perder ou passar
mais tempo do que o necessário tentando coletar uma
infinidade de páginas geradas dinamicamente. Ignorar o conteúdo do
componente \emph{query} de URLs é uma forma de tentar contornar esse
problema mas está longe de ser uma solução eficaz, uma vez que existem
formas de se apresentar conteúdo dinâmico sem que esse envolva o uso de
URLs com componentes \emph{query}.

Uma outra forma de gerar conteúdo ou URLs dinâmicas é através do
componente \emph{domainname} de URLs. Essa forma é particularmente
nociva à nossa implementação, sobretudo na que usa a política de
escalonamento BF. Isso deve-se a diversos fatores:
\begin{itemize}
\item Quando uma página em um domínio desses é encontrada pela primeira
vez, acaba-se descobrindo por tabela diversas páginas em diversos
domínios dinâmicos pertencentes ao mesmo sítio;
\item uma vez que cada novo domínio gerado
acarreta a criação de uma instância de gerente de domínio em nossa
implementação, ao visitar apenas uma única página num sítio desses
diversas instâncias de gerentes de domínios terão de ser geradas
instantaneamente,
\item finalmente,  como nessa política os domínios são visitados por ordem de
descoberta, o inicio da coleta do primeiro domínio dinâmico de um sítio
desses acarretará diversas requisições DNS, uma para cada um dos outros
domínios dinâmicos encontrados em seqüência.
\end{itemize}

Nessas circunstâncias o desempenho do coletor cai para patamares
baixíssimos, como pode-se observam em alguns vales na
Figura~\ref{fig:bf}. A política de escalonamento LSF contorna isso de
maneira elegante, mas até que ponto? Se tivessemos deixado o coletor com
LSF rodando por mais tempo teríamos encontrado esse tipo de
comportamento?

%Buracos negros - nossa abordagem de escalonamento foi baseada em domínios. Isso
%permitia não somente controlar o tempo mínimo de intervalo entre acessos
%consecutívos a um mesmo domínio, limitar o uso de memória (já que nao era mais
%necesári armazenar o nome de domínio em cada URL pertencente a ele) e armazenar
%a lista de URLs pendentes. Todavia, para a nossa implementação, sítios que
%fazem uso extensivo de sub-domínios para sua organização são torna um problema.
%Isso deve-se ao fato de que, custumeiramente, a obtenção e \emph{parsing} de
%uma unica página de um sítio destes acarreta na ``descoberta'' de vários
%sub-domínos desse sítio, o que por sua vez acarreta no acréscimo de cada um
%destes na lista de domínios conhecidos e de domínios pendentes. Além desses
%domínios serem costumeriramente rasos, ou seja, sem muitos links para
%documentos internos, a sua adição limitará a capacidade do crawler de baixar
%páginas de conteúdo, uma vez que, antes de que qualquer conteúdo possa ser
%obtido desses domínios, os arquivos \texttt{robots.txt} dele terá de ser
%obtido.

\subsubsection{Esgotamento da memória primária}

Todas as estruturas de controle de nossa implementação são deixadas em
memória primária. Em questão de horas isso começa a se tornar um
problema, quando o S.O. inicia um processo de colocar algumas das
páginas de memória de nosso sistema em  memória secundária. A medida que
o tempo passa, vai se tornando cada vez mais difícil não ser afetado
pela troca de páginas entre a memória principal e o \emph{swap}, fruto
do consumo de memória de nossa aplicação.

Uma forma de contornar o uso de memória seria fazer um uso mais
agressivo de identificadores numéricos para as páginas. Isso será útil
não somente para a contenção dos gastos com memória mas também quando
estivermos realizando a indexação dos documentos.

Encontrar mecanismos que nos permitam determinar a lista de URLs
pendentes em um dominio além de rapidamente determinar se uma
determinada URL já foi encontrada antes ou não sem depender tanto de
memória primária é um problema não resolvido de nossa implementação.
O uso de soluções como \emph{hashtables} distribuídas foi cogitado, mas
não chegamos a implementá-lo.

\subsubsection{Melhores implementações de listas de prioridades}

A adoção da política de escalonamento LSF mostrou-se bastante benéfica à
nossa implementação. Todavia, da forma que ela está implementada, ela
sofre de uma severa limitação: uma vez que um domínio tenha entrado na
lista de domínios ativos (Seção~\ref{sec:datastructures}), o valor do
tamanho da sua fila de páginas pendentes não pode mais ser atualizado.
Isso ocorre porque os algoritmos e estruturas disponíveis na STL, a
bilbioteca padrão de tipos e algoritmos do C++, para lidar com listas de
prioridades e \emph{heaps} não possuem métodos para decrementar ou
incrementar um o valor atribuído a um ítem interno ao \emph{heap}. Essa
limitação dificulta e limita também a implementação de políticas de
escalonamento de domínios ou de páginas baseados na quantidade de
\emph{links} que apontam para os mesmos.

Um solução seria implementar \emph{heaps} binários e de Fibonacci do
zero, de tal forma que esses possuíssem métodos para decrementar o valor
associado a um ítem ou atualizá-lo dinamicamente. Foi cogitada a
implementação uma estrutura similar a um \emph{heap} de Fibonacci mas
com custos assintóticos inferiores, conhecida como \emph{relaxed heap},
mas, por questões de tempo, essa implementação não foi
possível~\cite{driscoll1988relaxed}.

%%%%%%%%%%%%%%%%%%%%%%%%%% Bibliografia %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle {plain}
\bibliography{relatorio}


\end{document}

% vim:tw=72 fileencoding=utf-8 spelllang=pt spell syn=tex:
